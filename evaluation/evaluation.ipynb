{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1139,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_1 = \"flights_20241114_101306\"\n",
    "output_file_2 = \"flights_20241114_103536\"\n",
    "output_file_3 = \"flights_20241114_101306\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consolidated Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1140,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_1}/consolidated_error_annotations.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "output_2 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_2}/consolidated_error_annotations.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "output_3 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_3}/consolidated_error_annotations.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset = pd.read_csv(f\"../backend/data/{output_file_1}/clean.csv\")\n",
    "dirty_dataset = pd.read_csv(f\"../backend/data/{output_file_1}/dirty.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_errors(\n",
    "    df_fixed: pd.DataFrame, df_with_errors: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    # Check if the dataframes have the same shape and columns after sorting\n",
    "    if df_fixed.shape != df_with_errors.shape or not all(\n",
    "        df_fixed.columns == df_with_errors.columns\n",
    "    ):\n",
    "        raise ValueError(\"Both dataframes must have the same structure.\")\n",
    "\n",
    "    # Convert both dataframes to strings for datatype-agnostic comparison\n",
    "    df_fixed_str = df_fixed.astype(str)\n",
    "    df_with_errors_str = df_with_errors.astype(str)\n",
    "\n",
    "    # Create the annotation dataframe by comparing the two dataframes\n",
    "    error_annotation = (df_fixed_str != df_with_errors_str).astype(int)\n",
    "\n",
    "    return error_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1143,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_annotation = annotate_errors(clean_dataset, dirty_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_method(df1, df2, df3):\n",
    "    \"\"\"\n",
    "    Combine three dataframes using the union method.\n",
    "    A cell is 1 in the output if it is 1 in any of the input dataframes.\n",
    "    \"\"\"\n",
    "    return (df1 | df2 | df3).astype(int)\n",
    "\n",
    "\n",
    "def threshold_method(df1, df2, df3, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Combine three dataframes using a threshold.\n",
    "    A cell is 1 in the output if it is 1 in at least `threshold` proportion of the input dataframes.\n",
    "    \"\"\"\n",
    "    # Stack the dataframes and calculate the sum along the stack\n",
    "    stacked = np.stack([df1.values, df2.values, df3.values])\n",
    "    count_ones = np.sum(stacked, axis=0)\n",
    "\n",
    "    # Calculate the threshold in terms of number of dataframes\n",
    "    threshold_count = int(threshold * 3)\n",
    "\n",
    "    # Determine if each cell meets the threshold and create a DataFrame\n",
    "    result = (count_ones >= threshold_count).astype(int)\n",
    "\n",
    "    # Return the result as a DataFrame with the same index and columns as the input dataframes\n",
    "    return pd.DataFrame(result, index=df1.index, columns=df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_metrics(true_dataset: pd.DataFrame, pred_dataset: pd.DataFrame):\n",
    "    # Flatten the dataframes to 1D arrays\n",
    "    y_true = true_dataset.values.flatten()\n",
    "    y_pred = pred_dataset.values.flatten()\n",
    "\n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Handle precision, recall, and F1 score gracefully\n",
    "    try:\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    except ValueError:\n",
    "        # This case happens when y_true has no positive samples\n",
    "        precision = recall = f1 = 0.0\n",
    "\n",
    "    # Class-specific accuracy\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if cm.size == 4:  # If confusion matrix has 4 elements (2x2 matrix)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    else:  # Handle cases where there's only one class in y_true\n",
    "        tn = fp = fn = tp = 0\n",
    "        if len(cm) == 1:\n",
    "            tn = cm[0, 0] if y_true[0] == 0 else 0\n",
    "            tp = cm[0, 0] if y_true[0] == 1 else 0\n",
    "\n",
    "    # AUC scores\n",
    "    if len(set(y_true)) > 1:  # AUC scores require at least two classes\n",
    "        roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    else:\n",
    "        roc_auc = None\n",
    "\n",
    "    # Count of 1s in true and predicted labels\n",
    "    predicted_positives_count = sum(y_pred == 1)  # Total 1s in true labels\n",
    "    actual_positives_count = sum(y_true == 1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"roc_auc\": float(roc_auc) if roc_auc is not None else None,\n",
    "        \"true_positives_count\": int(tp),\n",
    "        \"true_negative_count\": int(tn),\n",
    "        \"false_positive_count\": int(fp),\n",
    "        \"false_negative_count\": int(fn),\n",
    "        \"predicted_positives_count\": int(predicted_positives_count),\n",
    "        \"actual_positives_count\": int(actual_positives_count),\n",
    "        \"fp_rate\": float(fp / len(y_pred)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-wise metric calculation wrapper\n",
    "def calculate_columnwise_metrics(\n",
    "    true_dataset: pd.DataFrame, pred_dataset: pd.DataFrame\n",
    "):\n",
    "    results = {}\n",
    "\n",
    "    for column in true_dataset.columns:\n",
    "        if column in pred_dataset.columns:\n",
    "\n",
    "            metrics = calculate_metrics(true_dataset[[column]], pred_dataset[[column]])\n",
    "            results[column] = metrics\n",
    "        else:\n",
    "            results[column] = \"Column missing in predictions\"\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_output = union_method(output_1, output_2, output_3)\n",
    "\n",
    "threshold_output = threshold_method(output_1, output_2, output_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5573593073593074, 'precision': 0.3241647465437788, 'recall': 0.45752032520325203, 'f1_score': 0.37946729602157786, 'roc_auc': 0.5284100942956151, 'true_positives_count': 2251, 'true_negative_count': 7019, 'false_positive_count': 4693, 'false_negative_count': 2669, 'predicted_positives_count': 6944, 'actual_positives_count': 4920, 'fp_rate': 0.2821669071669072}\n"
     ]
    }
   ],
   "source": [
    "print(calculate_metrics(error_annotation, union_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5573593073593074, 'precision': 0.3241647465437788, 'recall': 0.45752032520325203, 'f1_score': 0.37946729602157786, 'roc_auc': 0.5284100942956151, 'true_positives_count': 2251, 'true_negative_count': 7019, 'false_positive_count': 4693, 'false_negative_count': 2669, 'predicted_positives_count': 6944, 'actual_positives_count': 4920, 'fp_rate': 0.2821669071669072}\n"
     ]
    }
   ],
   "source": [
    "print(calculate_metrics(error_annotation, threshold_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_columnwise_metrics(error_annotation, union_output)\n",
    "\n",
    "pd.DataFrame(results).to_csv(\"./consolidated_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = pd.read_csv(f\"../backend/data/{output_file_1}/attribute/output.csv\")\n",
    "\n",
    "output_2 = pd.read_csv(f\"../backend/data/{output_file_2}/attribute/output.csv\")\n",
    "\n",
    "output_3 = pd.read_csv(f\"../backend/data/{output_file_3}/attribute/output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_output = union_method(output_1, output_2, output_3)\n",
    "\n",
    "threshold_output = threshold_method(output_1, output_2, output_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.545995670995671, 'precision': 0.12252510760401722, 'recall': 0.08678861788617886, 'f1_score': 0.10160618679357526, 'roc_auc': 0.4128444455551113, 'true_positives_count': 427, 'true_negative_count': 8654, 'false_positive_count': 3058, 'false_negative_count': 4493, 'predicted_positives_count': 3485, 'actual_positives_count': 4920, 'fp_rate': 0.18386243386243387}\n"
     ]
    }
   ],
   "source": [
    "print(calculate_metrics(error_annotation, union_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.545995670995671, 'precision': 0.12252510760401722, 'recall': 0.08678861788617886, 'f1_score': 0.10160618679357526, 'roc_auc': 0.4128444455551113, 'true_positives_count': 427, 'true_negative_count': 8654, 'false_positive_count': 3058, 'false_negative_count': 4493, 'predicted_positives_count': 3485, 'actual_positives_count': 4920, 'fp_rate': 0.18386243386243387}\n"
     ]
    }
   ],
   "source": [
    "print(calculate_metrics(error_annotation, threshold_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_columnwise_metrics(error_annotation, union_output)\n",
    "\n",
    "pd.DataFrame(results).to_csv(\"./attribute_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency Violation Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_1}/dependency_violations/output.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "output_2 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_2}/dependency_violations/output.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "output_3 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_3}/dependency_violations/output.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_output = union_method(output_1, output_2, output_3)\n",
    "\n",
    "threshold_output = threshold_method(output_1, output_2, output_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7285954785954786, 'precision': 0.5507753876938469, 'recall': 0.4475609756097561, 'f1_score': 0.49383269791433054, 'roc_auc': 0.6471069905371184, 'true_positives_count': 2202, 'true_negative_count': 9916, 'false_positive_count': 1796, 'false_negative_count': 2718, 'predicted_positives_count': 3998, 'actual_positives_count': 4920, 'fp_rate': 0.10798460798460799}\n"
     ]
    }
   ],
   "source": [
    "print(calculate_metrics(error_annotation, union_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7285954785954786, 'precision': 0.5507753876938469, 'recall': 0.4475609756097561, 'f1_score': 0.49383269791433054, 'roc_auc': 0.6471069905371184, 'true_positives_count': 2202, 'true_negative_count': 9916, 'false_positive_count': 1796, 'false_negative_count': 2718, 'predicted_positives_count': 3998, 'actual_positives_count': 4920, 'fp_rate': 0.10798460798460799}\n"
     ]
    }
   ],
   "source": [
    "print(calculate_metrics(error_annotation, threshold_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_columnwise_metrics(error_annotation, union_output)\n",
    "\n",
    "pd.DataFrame(results).to_csv(\"./dep_viol_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totals for each dataset:\n",
      "   completion_tokens  prompt_tokens  total_tokens  batches  duration_seconds  \\\n",
      "0            94546.0       161716.0      256262.0     56.0           666.109   \n",
      "1            95313.0       161716.0      257029.0     56.0           672.702   \n",
      "2            97663.0       161716.0      259379.0     56.0           697.638   \n",
      "\n",
      "    duration  \n",
      "0  11:06.109  \n",
      "1  11:12.702  \n",
      "2  11:37.638  \n",
      "\n",
      "Average Duration Across Datasets:\n",
      "11:18.816\n",
      "257556.66666666666\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "measure = \"attribute\"\n",
    "# measure = \"dependency\"\n",
    "# measure = \"dependency_violations\"\n",
    "\n",
    "\n",
    "# Load your DataFrames\n",
    "df1 = pd.read_csv(\n",
    "    f\"../backend/data/rayyan_20241114_105931/{measure}/prompt_metadata.csv\"\n",
    ")\n",
    "df2 = pd.read_csv(\n",
    "    f\"../backend/data/rayyan_20241114_111119/{measure}/prompt_metadata.csv\"\n",
    ")\n",
    "df3 = pd.read_csv(\n",
    "    f\"../backend/data/rayyan_20241114_112300/{measure}/prompt_metadata.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convert duration from MM:SS.MSS to seconds\n",
    "def duration_to_seconds(duration):\n",
    "    try:\n",
    "        # Split minutes and seconds\n",
    "        minutes, seconds = duration.split(\":\")\n",
    "        minutes = int(minutes)\n",
    "        seconds = float(seconds)  # Includes fractional seconds\n",
    "        return minutes * 60 + seconds\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing duration '{duration}': {e}\")\n",
    "        return 0  # Default to 0 seconds for invalid formats\n",
    "\n",
    "\n",
    "# Apply conversion to all DataFrames\n",
    "for df in [df1, df2, df3]:\n",
    "    df[\"duration_seconds\"] = df[\"elapsed_time\"].apply(duration_to_seconds)\n",
    "\n",
    "\n",
    "# Calculate totals for each dataset\n",
    "def calculate_totals(df):\n",
    "    totals = df.sum(numeric_only=True)  # Sum only numeric columns\n",
    "    totals[\"duration_seconds\"] = df[\"duration_seconds\"].sum()\n",
    "    return totals\n",
    "\n",
    "\n",
    "totals1 = calculate_totals(df1)\n",
    "totals2 = calculate_totals(df2)\n",
    "totals3 = calculate_totals(df3)\n",
    "\n",
    "# Combine totals into a single DataFrame\n",
    "totals_df = pd.DataFrame([totals1, totals2, totals3])\n",
    "\n",
    "# Calculate the average across datasets\n",
    "averages = totals_df.mean()\n",
    "\n",
    "\n",
    "# Convert total duration per dataset and average duration to MM:SS.MSS format\n",
    "def seconds_to_duration(seconds):\n",
    "    minutes = int(seconds // 60)\n",
    "    seconds = seconds % 60\n",
    "    return f\"{minutes:02}:{seconds:06.3f}\"  # Keeps milliseconds in the output\n",
    "\n",
    "\n",
    "# Convert durations\n",
    "totals_df[\"duration\"] = totals_df[\"duration_seconds\"].apply(seconds_to_duration)\n",
    "average_duration = seconds_to_duration(averages[\"duration_seconds\"])\n",
    "\n",
    "# Print results\n",
    "print(\"Totals for each dataset:\")\n",
    "print(totals_df)\n",
    "print(\"\\nAverage Duration Across Datasets:\")\n",
    "print(average_duration)\n",
    "print(averages[\"total_tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novel Detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C\n",
      "0  0.0  0.0  5.0\n",
      "1  0.0  0.0  0.0\n",
      "2  3.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example DataFrames\n",
    "df1 = pd.DataFrame({\"A\": [1, 0, 3], \"B\": [0, 4, 0], \"C\": [5, 0, 0]})\n",
    "\n",
    "df2 = pd.DataFrame({\"A\": [1, 0, 0], \"B\": [0, 4, 2], \"C\": [0, 1, 0]})\n",
    "\n",
    "# Step 1: Find where the values differ\n",
    "difference_mask = df1 != df2\n",
    "\n",
    "# Step 2: Create a DataFrame with only differing values\n",
    "differences = df1.where(difference_mask, np.nan)\n",
    "\n",
    "# Step 3: (Optional) Replace NaN with 0 if you want a 0-filled output\n",
    "differences_filled = differences.fillna(0)\n",
    "\n",
    "\n",
    "print(differences_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_classification(\n",
    "    true_dataset: pd.DataFrame, pred_dataset: pd.DataFrame, input_dataset: pd.DataFrame\n",
    "):\n",
    "    true_dataset.reset_index(drop=True)\n",
    "    pred_dataset.reset_index(drop=True)\n",
    "    input_dataset.reset_index(drop=True)\n",
    "\n",
    "    true_dataset.columns = input_dataset.columns\n",
    "    pred_dataset.columns = input_dataset.columns\n",
    "\n",
    "    calc = true_dataset.add(2)\n",
    "    calc_out = pred_dataset.copy()\n",
    "    calc_out[calc_out == 0] = -1\n",
    "\n",
    "    calc = calc.add(calc_out)\n",
    "\n",
    "    # True positive calculation\n",
    "    tp = calc == 4\n",
    "    true_positive_df = input_dataset[tp].astype(str)\n",
    "    true_positive_df = true_positive_df.replace(to_replace=\"nan\", value=0)\n",
    "    true_positive_df = true_positive_df.reset_index(drop=True)  # Remove index\n",
    "\n",
    "    # False positive calculation\n",
    "    fp = calc == 3\n",
    "    false_positive_df = input_dataset[fp].astype(str)\n",
    "    false_positive_df = false_positive_df.replace(to_replace=\"nan\", value=0)\n",
    "    false_positive_df = false_positive_df.reset_index(drop=True)  # Remove index\n",
    "\n",
    "    # False negative calculation\n",
    "    fn = calc == 2\n",
    "    false_negative_df = input_dataset[fn].astype(str)\n",
    "    false_negative_df = false_negative_df.replace(to_replace=\"nan\", value=0)\n",
    "    false_negative_df = false_negative_df.reset_index(drop=True)  # Remove index\n",
    "\n",
    "    all_errors_df = input_dataset[fp | fn].astype(str)\n",
    "    all_errors_df = all_errors_df.replace(to_replace=\"nan\", value=0)\n",
    "    all_errors_df = all_errors_df.reset_index(drop=True)  # Remove index\n",
    "\n",
    "    return true_positive_df, false_positive_df, false_negative_df, all_errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_1 = \"rayyan_20241114_105931\"\n",
    "output_file_2 = \"rayyan_20241114_111119\"\n",
    "output_file_3 = \"rayyan_20241114_112300\"\n",
    "\n",
    "output_1 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_1}/consolidated_error_annotations.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "output_2 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_2}/consolidated_error_annotations.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "output_3 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_3}/consolidated_error_annotations.csv\"\n",
    ")\n",
    "\n",
    "union_output = union_method(output_1, output_2, output_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"rayyan\"\n",
    "\n",
    "raha = (\n",
    "    pd.read_csv(f\"./tools/raha/datasets/{dataset}/annotated_cells.csv\")\n",
    "    # pd.read_csv(f\"./tools/raha/datasets/movies_1/annotated_cells.csv\")\n",
    "    .astype(int).fillna(0)\n",
    ")\n",
    "# SynODC output\n",
    "syn = (\n",
    "    pd.read_csv(f\"./tools/SynODC/Results/{dataset}/output/annotated_output.csv\")\n",
    "    .astype(int)\n",
    "    .fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "metadata": {},
   "outputs": [],
   "source": [
    "raha_synodc = (raha | syn).fillna(0)\n",
    "\n",
    "# Make sure the columns are in the correct order and ensure no index changes\n",
    "raha_synodc = raha_synodc.astype(int)\n",
    "\n",
    "# If you want to ensure the column order is preserved explicitly, you can reorder:\n",
    "raha_synodc = raha_synodc[raha.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1167,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_dataset = pd.read_csv(f\"./datasets/{dataset}/{dataset}.csv\")\n",
    "clean_dataset = pd.read_csv(f\"./datasets/{dataset}/clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_annotation = annotate_errors(clean_dataset, dirty_dataset)\n",
    "\n",
    "# CAED output\n",
    "true_positive_df, false_positive_df, false_negative_df, all_errors_df = (\n",
    "    inspect_classification(error_annotation, union_output, dirty_dataset)\n",
    ")\n",
    "\n",
    "caed_tp = true_positive_df.copy()\n",
    "\n",
    "\n",
    "# Raha SynODC output\n",
    "true_positive_df, false_positive_df, false_negative_df, all_errors_df = (\n",
    "    inspect_classification(error_annotation, raha_synodc, dirty_dataset)\n",
    ")\n",
    "\n",
    "raha_syn_tp = true_positive_df.copy()\n",
    "\n",
    "# Raha output\n",
    "true_positive_df, false_positive_df, false_negative_df, all_errors_df = (\n",
    "    inspect_classification(error_annotation, raha, dirty_dataset)\n",
    ")\n",
    "\n",
    "raha_tp = true_positive_df.copy()\n",
    "\n",
    "# SynODC output\n",
    "true_positive_df, false_positive_df, false_negative_df, all_errors_df = (\n",
    "    inspect_classification(error_annotation, syn, dirty_dataset)\n",
    ")\n",
    "\n",
    "syn_tp = true_positive_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of non-zero values: 253\n"
     ]
    }
   ],
   "source": [
    "# Combined\n",
    "# Step 1: Find where the values differ\n",
    "difference_mask = caed_tp != raha_syn_tp\n",
    "\n",
    "# Step 2: Create a DataFrame with only differing values\n",
    "differences = caed_tp.where(difference_mask, np.nan)\n",
    "\n",
    "# Step 3: Replace NaN with 0, ensuring all values are treated as float\n",
    "differences_filled = differences.fillna(0)\n",
    "\n",
    "# Optional: Drop the first column if needed\n",
    "# differences_filled.drop(columns=differences_filled.columns[0], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Step 4: Define a function to identify non-zero values robustly\n",
    "def is_non_zero(value):\n",
    "    try:\n",
    "        # Try to cast to a float, compare to zero\n",
    "        return float(value) != 0.0\n",
    "    except ValueError:\n",
    "        # If value cannot be converted to a float, assume it's non-zero\n",
    "        return True\n",
    "\n",
    "\n",
    "# Apply the function to the entire DataFrame\n",
    "non_zero_mask = differences_filled.applymap(is_non_zero)\n",
    "\n",
    "# Count the number of True values (non-zero values)\n",
    "non_zero_count = non_zero_mask.sum().sum()\n",
    "\n",
    "print(f\"Count of non-zero values: {non_zero_count}\")\n",
    "\n",
    "# Step 5: Save the differences\n",
    "differences_filled.to_csv(f\"./novel_detections/{dataset}/novel.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of non-zero values Raha: 256\n"
     ]
    }
   ],
   "source": [
    "# Raha output\n",
    "\n",
    "# Step 1: Find where the values differ\n",
    "difference_mask = caed_tp != raha_tp\n",
    "\n",
    "\n",
    "# Step 2: Create a DataFrame with only differing values\n",
    "differences = caed_tp.where(difference_mask, np.nan)\n",
    "\n",
    "\n",
    "# Step 3: Replace NaN with 0, ensuring all values are treated as float\n",
    "differences_filled = differences.fillna(0)\n",
    "\n",
    "\n",
    "# Optional: Drop the first column if needed\n",
    "# differences_filled.drop(columns=differences_filled.columns[0], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Step 4: Define a function to identify non-zero values robustly\n",
    "def is_non_zero(value):\n",
    "    try:\n",
    "        # Try to cast to a float, compare to zero\n",
    "        return float(value) != 0.0\n",
    "    except ValueError:\n",
    "        # If value cannot be converted to a float, assume it's non-zero\n",
    "        return True\n",
    "\n",
    "\n",
    "# Apply the function to the entire DataFrame\n",
    "non_zero_mask = differences_filled.applymap(is_non_zero)\n",
    "\n",
    "# Count the number of True values (non-zero values)\n",
    "non_zero_count = non_zero_mask.sum().sum()\n",
    "\n",
    "print(f\"Count of non-zero values Raha: {non_zero_count}\")\n",
    "\n",
    "# Step 5: Save the differences\n",
    "# differences_filled.to_csv(\"./novel.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of non-zero values SynODC: 516\n"
     ]
    }
   ],
   "source": [
    "# SynODC\n",
    "\n",
    "# Raha output\n",
    "# raha = pd.read_csv(f\"./tools/raha/datasets/{dataset}/tp.csv\")\n",
    "# # SynODC output\n",
    "\n",
    "\n",
    "# raha_syn = raha.combine_first(syn).iloc[:, 1:]\n",
    "\n",
    "\n",
    "# Step 1: Find where the values differ\n",
    "difference_mask = caed_tp != syn_tp\n",
    "\n",
    "# Step 2: Create a DataFrame with only differing values\n",
    "differences = caed_tp.where(difference_mask, np.nan)\n",
    "\n",
    "# Step 3: Replace NaN with 0, ensuring all values are treated as float\n",
    "differences_filled = differences.fillna(0)\n",
    "\n",
    "# Optional: Drop the first column if needed\n",
    "# differences_filled.drop(columns=differences_filled.columns[0], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Step 4: Define a function to identify non-zero values robustly\n",
    "def is_non_zero(value):\n",
    "    try:\n",
    "        # Try to cast to a float, compare to zero\n",
    "        return float(value) != 0.0\n",
    "    except ValueError:\n",
    "        # If value cannot be converted to a float, assume it's non-zero\n",
    "        return True\n",
    "\n",
    "\n",
    "# Apply the function to the entire DataFrame\n",
    "non_zero_mask = differences_filled.applymap(is_non_zero)\n",
    "\n",
    "# Count the number of True values (non-zero values)\n",
    "non_zero_count = non_zero_mask.sum().sum()\n",
    "\n",
    "print(f\"Count of non-zero values SynODC: {non_zero_count}\")\n",
    "\n",
    "# Step 5: Save the differences\n",
    "# differences_filled.to_csv(\"./novel.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
