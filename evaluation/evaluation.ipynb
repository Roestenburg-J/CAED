{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAED Evaluation\n",
    "\n",
    "This notebook is used to evaluate the results obtained from CAED detections.\n",
    "\n",
    "It allows you to do the following:\n",
    "\n",
    "1. Create the probability based, and union based combined output for three seperate detections\n",
    "2. Calcualte error detections performance metrics\n",
    "3. Evaluate detections based on Attribute Prompting, and Depdencency Violation Prompting\n",
    "4. Calcualte cost performance metrics for detections\n",
    "5. Identify Novel detections\n",
    "\n",
    "**Note!** The paths used in this notebook is static. If folders are moved or changed in the repository, it might not function as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_1 = \"flights_20241114_101306\"\n",
    "output_file_2 = \"flights_20241114_103536\"\n",
    "output_file_3 = \"flights_20241114_101306\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consolidated Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_1}/consolidated_error_annotations.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "output_2 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_2}/consolidated_error_annotations.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "output_3 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_3}/consolidated_error_annotations.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset = pd.read_csv(f\"../backend/data/{output_file_1}/clean.csv\")\n",
    "dirty_dataset = pd.read_csv(f\"../backend/data/{output_file_1}/dirty.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_errors(\n",
    "    df_fixed: pd.DataFrame, df_with_errors: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    # Check if the dataframes have the same shape and columns after sorting\n",
    "    if df_fixed.shape != df_with_errors.shape or not all(\n",
    "        df_fixed.columns == df_with_errors.columns\n",
    "    ):\n",
    "        raise ValueError(\"Both dataframes must have the same structure.\")\n",
    "\n",
    "    # Convert both dataframes to strings for datatype-agnostic comparison\n",
    "    df_fixed_str = df_fixed.astype(str)\n",
    "    df_with_errors_str = df_with_errors.astype(str)\n",
    "\n",
    "    # Create the annotation dataframe by comparing the two dataframes\n",
    "    error_annotation = (df_fixed_str != df_with_errors_str).astype(int)\n",
    "\n",
    "    return error_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_annotation = annotate_errors(clean_dataset, dirty_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_method(df1, df2, df3):\n",
    "    \"\"\"\n",
    "    Combine three dataframes using the union method.\n",
    "    A cell is 1 in the output if it is 1 in any of the input dataframes.\n",
    "    \"\"\"\n",
    "    return (df1 | df2 | df3).astype(int)\n",
    "\n",
    "\n",
    "def threshold_method(df1, df2, df3, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Combine three dataframes using a threshold.\n",
    "    A cell is 1 in the output if it is 1 in at least `threshold` proportion of the input dataframes.\n",
    "    \"\"\"\n",
    "    # Stack the dataframes and calculate the sum along the stack\n",
    "    stacked = np.stack([df1.values, df2.values, df3.values])\n",
    "    count_ones = np.sum(stacked, axis=0)\n",
    "\n",
    "    # Calculate the threshold in terms of number of dataframes\n",
    "    threshold_count = int(threshold * 3)\n",
    "\n",
    "    # Determine if each cell meets the threshold and create a DataFrame\n",
    "    result = (count_ones >= threshold_count).astype(int)\n",
    "\n",
    "    # Return the result as a DataFrame with the same index and columns as the input dataframes\n",
    "    return pd.DataFrame(result, index=df1.index, columns=df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(true_dataset: pd.DataFrame, pred_dataset: pd.DataFrame):\n",
    "    # Flatten the dataframes to 1D arrays\n",
    "    y_true = true_dataset.values.flatten()\n",
    "    y_pred = pred_dataset.values.flatten()\n",
    "\n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Handle precision, recall, and F1 score gracefully\n",
    "    try:\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    except ValueError:\n",
    "        # This case happens when y_true has no positive samples\n",
    "        precision = recall = f1 = 0.0\n",
    "\n",
    "    # Class-specific accuracy\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if cm.size == 4:  # If confusion matrix has 4 elements (2x2 matrix)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    else:  # Handle cases where there's only one class in y_true\n",
    "        tn = fp = fn = tp = 0\n",
    "        if len(cm) == 1:\n",
    "            tn = cm[0, 0] if y_true[0] == 0 else 0\n",
    "            tp = cm[0, 0] if y_true[0] == 1 else 0\n",
    "\n",
    "    # AUC scores\n",
    "    if len(set(y_true)) > 1:  # AUC scores require at least two classes\n",
    "        roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    else:\n",
    "        roc_auc = None\n",
    "\n",
    "    # Count of 1s in true and predicted labels\n",
    "    predicted_positives_count = sum(y_pred == 1)  # Total 1s in true labels\n",
    "    actual_positives_count = sum(y_true == 1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"roc_auc\": float(roc_auc) if roc_auc is not None else None,\n",
    "        \"true_positives_count\": int(tp),\n",
    "        \"true_negative_count\": int(tn),\n",
    "        \"false_positive_count\": int(fp),\n",
    "        \"false_negative_count\": int(fn),\n",
    "        \"predicted_positives_count\": int(predicted_positives_count),\n",
    "        \"actual_positives_count\": int(actual_positives_count),\n",
    "        \"fp_rate\": float(fp / len(y_pred)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-wise metric calculation wrapper\n",
    "def calculate_columnwise_metrics(\n",
    "    true_dataset: pd.DataFrame, pred_dataset: pd.DataFrame\n",
    "):\n",
    "    results = {}\n",
    "\n",
    "    for column in true_dataset.columns:\n",
    "        if column in pred_dataset.columns:\n",
    "\n",
    "            metrics = calculate_metrics(true_dataset[[column]], pred_dataset[[column]])\n",
    "            results[column] = metrics\n",
    "        else:\n",
    "            results[column] = \"Column missing in predictions\"\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_output = union_method(output_1, output_2, output_3)\n",
    "\n",
    "threshold_output = threshold_method(output_1, output_2, output_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_metrics(error_annotation, union_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_metrics(error_annotation, threshold_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_columnwise_metrics(error_annotation, union_output)\n",
    "\n",
    "pd.DataFrame(results).to_csv(\"./consolidated_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = pd.read_csv(f\"../backend/data/{output_file_1}/attribute/output.csv\")\n",
    "\n",
    "output_2 = pd.read_csv(f\"../backend/data/{output_file_2}/attribute/output.csv\")\n",
    "\n",
    "output_3 = pd.read_csv(f\"../backend/data/{output_file_3}/attribute/output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_output = union_method(output_1, output_2, output_3)\n",
    "\n",
    "threshold_output = threshold_method(output_1, output_2, output_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_metrics(error_annotation, union_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_metrics(error_annotation, threshold_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_columnwise_metrics(error_annotation, union_output)\n",
    "\n",
    "pd.DataFrame(results).to_csv(\"./attribute_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency Violation Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_1}/dependency_violations/output.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "output_2 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_2}/dependency_violations/output.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "output_3 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_3}/dependency_violations/output.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_output = union_method(output_1, output_2, output_3)\n",
    "\n",
    "threshold_output = threshold_method(output_1, output_2, output_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_metrics(error_annotation, union_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_metrics(error_annotation, threshold_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_columnwise_metrics(error_annotation, union_output)\n",
    "\n",
    "pd.DataFrame(results).to_csv(\"./dep_viol_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment measure to easily rotate between times of detection\n",
    "\n",
    "# measure = \"attribute\"\n",
    "# measure = \"dependency\"\n",
    "measure = \"dependency_violations\"\n",
    "\n",
    "\n",
    "# Load your DataFrames\n",
    "df1 = pd.read_csv(\n",
    "    f\"../backend/data/rayyan_20241114_105931/{measure}/prompt_metadata.csv\"\n",
    ")\n",
    "df2 = pd.read_csv(\n",
    "    f\"../backend/data/rayyan_20241114_111119/{measure}/prompt_metadata.csv\"\n",
    ")\n",
    "df3 = pd.read_csv(\n",
    "    f\"../backend/data/rayyan_20241114_112300/{measure}/prompt_metadata.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convert duration from MM:SS.MSS to seconds\n",
    "def duration_to_seconds(duration):\n",
    "    try:\n",
    "        # Split minutes and seconds\n",
    "        minutes, seconds = duration.split(\":\")\n",
    "        minutes = int(minutes)\n",
    "        seconds = float(seconds)  # Includes fractional seconds\n",
    "        return minutes * 60 + seconds\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing duration '{duration}': {e}\")\n",
    "        return 0  # Default to 0 seconds for invalid formats\n",
    "\n",
    "\n",
    "# Apply conversion to all DataFrames\n",
    "for df in [df1, df2, df3]:\n",
    "    df[\"duration_seconds\"] = df[\"elapsed_time\"].apply(duration_to_seconds)\n",
    "\n",
    "\n",
    "# Calculate totals for each dataset\n",
    "def calculate_totals(df):\n",
    "    totals = df.sum(numeric_only=True)  # Sum only numeric columns\n",
    "    totals[\"duration_seconds\"] = df[\"duration_seconds\"].sum()\n",
    "    return totals\n",
    "\n",
    "\n",
    "totals1 = calculate_totals(df1)\n",
    "totals2 = calculate_totals(df2)\n",
    "totals3 = calculate_totals(df3)\n",
    "\n",
    "# Combine totals into a single DataFrame\n",
    "totals_df = pd.DataFrame([totals1, totals2, totals3])\n",
    "\n",
    "# Calculate the average across datasets\n",
    "averages = totals_df.mean()\n",
    "\n",
    "\n",
    "# Convert total duration per dataset and average duration to MM:SS.MSS format\n",
    "def seconds_to_duration(seconds):\n",
    "    minutes = int(seconds // 60)\n",
    "    seconds = seconds % 60\n",
    "    return f\"{minutes:02}:{seconds:06.3f}\"  # Keeps milliseconds in the output\n",
    "\n",
    "\n",
    "# Convert durations\n",
    "totals_df[\"duration\"] = totals_df[\"duration_seconds\"].apply(seconds_to_duration)\n",
    "average_duration = seconds_to_duration(averages[\"duration_seconds\"])\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"Totals for each dataset:\")\n",
    "print(totals_df)\n",
    "print(\"\\nAverage Duration Across Datasets:\")\n",
    "print(average_duration)\n",
    "print(\"Total Tokens: \", averages[\"total_tokens\"])\n",
    "print(\"Completion Tokens: \", averages[\"completion_tokens\"])\n",
    "print(\"Prompt Tokens: \", averages[\"prompt_tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost calcualtion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input cost\n",
    "\n",
    "tokens = 176359\n",
    "\n",
    "cost = 0.15\n",
    "\n",
    "\n",
    "\n",
    "print(\"$\", float(tokens / 1000000) * cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output cost\n",
    "\n",
    "tokens = 29818\n",
    "\n",
    "cost = 0.60\n",
    "\n",
    "\n",
    "\n",
    "print(\"$\", float(tokens / 1000000) * cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novel Detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_classification(\n",
    "    true_dataset: pd.DataFrame, pred_dataset: pd.DataFrame, input_dataset: pd.DataFrame\n",
    "):\n",
    "    true_dataset.reset_index(drop=True)\n",
    "    pred_dataset.reset_index(drop=True)\n",
    "    input_dataset.reset_index(drop=True)\n",
    "\n",
    "    true_dataset.columns = input_dataset.columns\n",
    "    pred_dataset.columns = input_dataset.columns\n",
    "\n",
    "    calc = true_dataset.add(2)\n",
    "    calc_out = pred_dataset.copy()\n",
    "    calc_out[calc_out == 0] = -1\n",
    "\n",
    "    calc = calc.add(calc_out)\n",
    "\n",
    "    # True positive calculation\n",
    "    tp = calc == 4\n",
    "    true_positive_df = input_dataset[tp].astype(str)\n",
    "    true_positive_df = true_positive_df.replace(to_replace=\"nan\", value=0)\n",
    "    true_positive_df = true_positive_df.reset_index(drop=True)  # Remove index\n",
    "\n",
    "    # False positive calculation\n",
    "    fp = calc == 3\n",
    "    false_positive_df = input_dataset[fp].astype(str)\n",
    "    false_positive_df = false_positive_df.replace(to_replace=\"nan\", value=0)\n",
    "    false_positive_df = false_positive_df.reset_index(drop=True)  # Remove index\n",
    "\n",
    "    # False negative calculation\n",
    "    fn = calc == 2\n",
    "    false_negative_df = input_dataset[fn].astype(str)\n",
    "    false_negative_df = false_negative_df.replace(to_replace=\"nan\", value=0)\n",
    "    false_negative_df = false_negative_df.reset_index(drop=True)  # Remove index\n",
    "\n",
    "    all_errors_df = input_dataset[fp | fn].astype(str)\n",
    "    all_errors_df = all_errors_df.replace(to_replace=\"nan\", value=0)\n",
    "    all_errors_df = all_errors_df.reset_index(drop=True)  # Remove index\n",
    "\n",
    "    return true_positive_df, false_positive_df, false_negative_df, all_errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_1 = \"rayyan_20241114_105931\"\n",
    "output_file_2 = \"rayyan_20241114_111119\"\n",
    "output_file_3 = \"rayyan_20241114_112300\"\n",
    "\n",
    "output_1 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_1}/consolidated_error_annotations.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "output_2 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_2}/consolidated_error_annotations.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "output_3 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_3}/consolidated_error_annotations.csv\"\n",
    ")\n",
    "\n",
    "union_output = union_method(output_1, output_2, output_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"rayyan\"\n",
    "\n",
    "raha = (\n",
    "    pd.read_csv(f\"./tools/raha/datasets/{dataset}/annotated_cells.csv\")\n",
    "    # pd.read_csv(f\"./tools/raha/datasets/movies_1/annotated_cells.csv\")\n",
    "    .astype(int).fillna(0)\n",
    ")\n",
    "# SynODC output\n",
    "syn = (\n",
    "    pd.read_csv(f\"./tools/SynODC/Results/{dataset}/output/annotated_output.csv\")\n",
    "    .astype(int)\n",
    "    .fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raha_synodc = (raha | syn).fillna(0)\n",
    "\n",
    "# Make sure the columns are in the correct order and ensure no index changes\n",
    "raha_synodc = raha_synodc.astype(int)\n",
    "\n",
    "# If you want to ensure the column order is preserved explicitly, you can reorder:\n",
    "raha_synodc = raha_synodc[raha.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_dataset = pd.read_csv(f\"./datasets/{dataset}/{dataset}.csv\")\n",
    "clean_dataset = pd.read_csv(f\"./datasets/{dataset}/clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_annotation = annotate_errors(clean_dataset, dirty_dataset)\n",
    "\n",
    "# CAED output\n",
    "true_positive_df, false_positive_df, false_negative_df, all_errors_df = (\n",
    "    inspect_classification(error_annotation, union_output, dirty_dataset)\n",
    ")\n",
    "\n",
    "caed_tp = true_positive_df.copy()\n",
    "\n",
    "\n",
    "# Raha SynODC output\n",
    "true_positive_df, false_positive_df, false_negative_df, all_errors_df = (\n",
    "    inspect_classification(error_annotation, raha_synodc, dirty_dataset)\n",
    ")\n",
    "\n",
    "raha_syn_tp = true_positive_df.copy()\n",
    "\n",
    "# Raha output\n",
    "true_positive_df, false_positive_df, false_negative_df, all_errors_df = (\n",
    "    inspect_classification(error_annotation, raha, dirty_dataset)\n",
    ")\n",
    "\n",
    "raha_tp = true_positive_df.copy()\n",
    "\n",
    "# SynODC output\n",
    "true_positive_df, false_positive_df, false_negative_df, all_errors_df = (\n",
    "    inspect_classification(error_annotation, syn, dirty_dataset)\n",
    ")\n",
    "\n",
    "syn_tp = true_positive_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined\n",
    "# Find where the values differ\n",
    "difference_mask = caed_tp != raha_syn_tp\n",
    "\n",
    "# Create a DataFrame with only differing values\n",
    "differences = caed_tp.where(difference_mask, np.nan)\n",
    "\n",
    "# Replace NaN with 0, ensuring all values are treated as float\n",
    "differences_filled = differences.fillna(0)\n",
    "\n",
    "\n",
    "# Define a function to identify non-zero values robustly\n",
    "def is_non_zero(value):\n",
    "    try:\n",
    "        # Try to cast to a float, compare to zero\n",
    "        return float(value) != 0.0\n",
    "    except ValueError:\n",
    "        # If value cannot be converted to a float, assume it's non-zero\n",
    "        return True\n",
    "\n",
    "\n",
    "# Apply the function to the entire DataFrame\n",
    "non_zero_mask = differences_filled.applymap(is_non_zero)\n",
    "\n",
    "# Count the number of True values (non-zero values)\n",
    "non_zero_count = non_zero_mask.sum().sum()\n",
    "\n",
    "print(f\"Count of non-zero values: {non_zero_count}\")\n",
    "\n",
    "# Save the differences\n",
    "differences_filled.to_csv(f\"./novel_detections/{dataset}/novel.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raha output\n",
    "\n",
    "# Find where the values differ\n",
    "difference_mask = caed_tp != raha_tp\n",
    "\n",
    "\n",
    "# Create a DataFrame with only differing values\n",
    "differences = caed_tp.where(difference_mask, np.nan)\n",
    "\n",
    "\n",
    "# Replace NaN with 0, ensuring all values are treated as float\n",
    "differences_filled = differences.fillna(0)\n",
    "\n",
    "\n",
    "# Define a function to identify non-zero values robustly\n",
    "def is_non_zero(value):\n",
    "    try:\n",
    "        # Try to cast to a float, compare to zero\n",
    "        return float(value) != 0.0\n",
    "    except ValueError:\n",
    "        # If value cannot be converted to a float, assume it's non-zero\n",
    "        return True\n",
    "\n",
    "\n",
    "# Apply the function to the entire DataFrame\n",
    "non_zero_mask = differences_filled.applymap(is_non_zero)\n",
    "\n",
    "# Count the number of True values (non-zero values)\n",
    "non_zero_count = non_zero_mask.sum().sum()\n",
    "\n",
    "print(f\"Count of non-zero values Raha: {non_zero_count}\")\n",
    "\n",
    "# Save the differences\n",
    "# differences_filled.to_csv(\"./novel.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SynODC\n",
    "\n",
    "# Find where the values differ\n",
    "difference_mask = caed_tp != syn_tp\n",
    "\n",
    "# Create a DataFrame with only differing values\n",
    "differences = caed_tp.where(difference_mask, np.nan)\n",
    "\n",
    "# Replace NaN with 0, ensuring all values are treated as float\n",
    "differences_filled = differences.fillna(0)\n",
    "\n",
    "\n",
    "# Define a function to identify non-zero values robustly\n",
    "def is_non_zero(value):\n",
    "    try:\n",
    "        # Try to cast to a float, compare to zero\n",
    "        return float(value) != 0.0\n",
    "    except ValueError:\n",
    "        # If value cannot be converted to a float, assume it's non-zero\n",
    "        return True\n",
    "\n",
    "\n",
    "# Apply the function to the entire DataFrame\n",
    "non_zero_mask = differences_filled.applymap(is_non_zero)\n",
    "\n",
    "# Count the number of True values (non-zero values)\n",
    "non_zero_count = non_zero_mask.sum().sum()\n",
    "\n",
    "print(f\"Count of non-zero values SynODC: {non_zero_count}\")\n",
    "\n",
    "# Save the differences\n",
    "# differences_filled.to_csv(\"./novel.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
