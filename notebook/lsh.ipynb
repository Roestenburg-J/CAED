{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./datasets/hospital/dirty.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "records = dataset.values.tolist()\n",
    "\n",
    "# Create an LSH index with a threshold and number of permutations\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "\n",
    "# Insert each record into the LSH index\n",
    "for i, record in enumerate(records):\n",
    "    m = MinHash(num_perm=128)\n",
    "    for feature in record:\n",
    "        m.update(str(feature).encode(\"utf8\"))  # Hashing the attributes of the record\n",
    "    lsh.insert(i, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store the buckets (groups of similar records)\n",
    "buckets = []\n",
    "visited = set()  # To track records that have already been assigned to a bucket\n",
    "\n",
    "# Querying similar records for each record\n",
    "for i, record in enumerate(records):\n",
    "    if i not in visited:  # Only process records that haven't been visited\n",
    "        # Create MinHash for the current record\n",
    "        m = MinHash(num_perm=128)\n",
    "        for feature in record:\n",
    "            m.update(str(feature).encode(\"utf8\"))\n",
    "\n",
    "        # Query LSH to get similar records\n",
    "        similar_records = lsh.query(m)\n",
    "\n",
    "        # Add the current record and its similar ones as a new bucket\n",
    "        buckets.append(similar_records)\n",
    "\n",
    "        # Mark all similar records as visited\n",
    "        visited.update(similar_records)\n",
    "\n",
    "# Print the buckets\n",
    "# for idx, bucket in enumerate(buckets):\n",
    "#     print(f\"Bucket {idx + 1}: {bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Buckets by Size (Minimum Size Threshold: 2):\n",
      "Bucket 29 (Size: 39): [896, 898, 899, 900, 416, 418, 419, 420, 423, 424, 426, 429, 195, 69, 70, 73, 74, 76, 77, 78, 80, 84, 90, 91, 348, 92, 352, 878, 881, 882, 883, 884, 885, 886, 887, 888, 889, 892, 895]\n",
      "Bucket 2 (Size: 27): [384, 8, 9, 10, 392, 396, 13, 11, 12, 14, 15, 17, 294, 295, 296, 452, 459, 460, 461, 463, 464, 465, 852, 853, 854, 986, 858]\n",
      "Bucket 0 (Size: 20): [0, 1, 2, 3, 4, 5, 6, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 308, 309, 310]\n",
      "Bucket 173 (Size: 20): [591, 592, 593, 594, 595, 596, 597, 598, 600, 603, 604, 607, 608, 609, 610, 611, 612, 613, 614, 615]\n",
      "Bucket 225 (Size: 20): [774, 775, 776, 778, 781, 783, 784, 785, 787, 789, 790, 792, 793, 794, 795, 796, 798, 799, 800, 801]\n",
      "Bucket 23 (Size: 19): [654, 529, 404, 790, 918, 282, 679, 815, 307, 57, 454, 326, 207, 479, 993, 232, 629, 504, 890]\n",
      "Bucket 53 (Size: 19): [169, 170, 171, 172, 173, 174, 175, 177, 178, 181, 182, 183, 184, 187, 188, 191, 192, 724, 741]\n",
      "Bucket 155 (Size: 19): [516, 517, 518, 519, 521, 522, 523, 524, 525, 527, 529, 530, 533, 534, 535, 536, 537, 538, 539]\n",
      "Bucket 230 (Size: 19): [802, 803, 804, 806, 807, 808, 809, 810, 812, 813, 817, 819, 821, 822, 823, 824, 825, 826, 741]\n",
      "Bucket 1 (Size: 18): [3, 4, 5, 6, 295, 296, 7, 298, 299, 300, 301, 294, 303, 304, 306, 307, 309, 310]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the sizes of all buckets\n",
    "bucket_sizes = [(i, len(bucket)) for i, bucket in enumerate(buckets)]\n",
    "\n",
    "# Sort buckets by size in descending order\n",
    "sorted_buckets = sorted(bucket_sizes, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Retrieve the top 10 buckets\n",
    "top_buckets_raw = sorted_buckets[:10]  # Get the top 10 buckets\n",
    "\n",
    "# Define a minimum size threshold (e.g., 2)\n",
    "min_size_threshold = 2\n",
    "\n",
    "# Filter the top buckets based on the minimum size threshold\n",
    "filtered_top_buckets = [\n",
    "    bucket for bucket in top_buckets_raw if bucket[1] >= min_size_threshold\n",
    "]\n",
    "\n",
    "# Print the filtered top buckets and their sizes\n",
    "print(f\"Top Buckets by Size (Minimum Size Threshold: {min_size_threshold}):\")\n",
    "for idx, size in filtered_top_buckets:\n",
    "    print(f\"Bucket {idx} (Size: {size}): {buckets[idx]}\")\n",
    "\n",
    "# If you want to handle the case where no buckets meet the threshold\n",
    "if not filtered_top_buckets:\n",
    "    print(\"No buckets met the minimum size threshold.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
