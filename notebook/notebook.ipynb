{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import time\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from typing import Dict, Any\n",
    "from datasketch import MinHash, MinHashLSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT API setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    organization=\"org-Efj3WwiBs01tiD9ogyAb1vgz\",\n",
    "    project=\"proj_ItFIKb0eOHXEFM65qPMVLpHt\",\n",
    "    api_key=\"sk-proj-waDJ9nwjNNOcQa6Ol0epT3BlbkFJbwwL9qZnqZmBMVCwtvOX\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the prompts used, and the GPT output to a folder for later inspection\n",
    "def write_output(\n",
    "    gpt_output: Dict[str, Any],\n",
    "    user_prompt: str,\n",
    "    system_prompt: str,\n",
    "    prompt_title: str,\n",
    "    directory: str,\n",
    "    input_data_dict: pd.DataFrame,\n",
    ") -> None:\n",
    "\n",
    "    try:\n",
    "        # Create the subdirectory for the pompt_title if it doesn't exist\n",
    "        prompt_directory = os.path.join(\n",
    "            directory, prompt_title.strip()\n",
    "        )  # Strip whitespace\n",
    "        os.makedirs(prompt_directory, exist_ok=True)\n",
    "\n",
    "        # Define the file paths\n",
    "        output_text_file = os.path.join(prompt_directory, \"output.json\")\n",
    "        user_prompt_text_file = os.path.join(prompt_directory, \"user_prompt.txt\")\n",
    "        sys_prompt_text_file = os.path.join(prompt_directory, \"system_prompt.txt\")\n",
    "        input_data_dict_file = os.path.join(prompt_directory, \"dict.csv\")\n",
    "\n",
    "        # Write the output\n",
    "        with open(output_text_file, \"w\") as f:\n",
    "            f.write(gpt_output)\n",
    "\n",
    "        # Write the prompt\n",
    "        with open(user_prompt_text_file, \"w\") as f:\n",
    "            f.write(user_prompt)\n",
    "\n",
    "        with open(sys_prompt_text_file, \"w\") as f:\n",
    "            f.write(system_prompt)\n",
    "\n",
    "        input_data_dict.to_csv(\n",
    "            input_data_dict_file, index=False, sep=\",\", lineterminator=\"\\n\"\n",
    "        )\n",
    "\n",
    "        print(f\"Output and prompt saved in directory: {prompt_directory}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the number of tokens usd, the time to retrieve the output and other metadata to a CSV\n",
    "def write_prompt_metadata(\n",
    "    completion_tokens: int,\n",
    "    prompt_tokens: int,\n",
    "    total_tokens: int,\n",
    "    elapsed_time: str,\n",
    "    directory: str,\n",
    "    prompt_title: str,\n",
    ") -> None:\n",
    "    current_data = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"Completion Tokens\": completion_tokens,\n",
    "                \"Prompt Tokens\": prompt_tokens,\n",
    "                \"Total Tokens\": total_tokens,\n",
    "                \"Elapsed Time\": elapsed_time,\n",
    "                \"Prompt Name\": prompt_title,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prompt_metadata_path = os.path.join(directory, \"prompt_metadata.csv\")\n",
    "\n",
    "    # Check if the file exists to determine whether to write the header\n",
    "    if os.path.exists(prompt_metadata_path):\n",
    "        current_data.to_csv(prompt_metadata_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        current_data.to_csv(prompt_metadata_path, mode=\"w\", header=True, index=False)\n",
    "\n",
    "    print(\"Processing complete. Results and token/time data have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pompt of the GPT, providing one system prompt, a user prompt, and the expected schema for the output\n",
    "def prompt_gpt(\n",
    "    system_prompt: str,\n",
    "    user_prompt: str,\n",
    "    prompt_title: str,\n",
    "    response_format: dict[str],\n",
    "    output_directory: str,\n",
    "    input_data_dict: pd.DataFrame,\n",
    ") -> None:\n",
    "\n",
    "    class Output(BaseModel):\n",
    "        output: list[str]\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            max_tokens=16000,\n",
    "            response_format=response_format,\n",
    "        )\n",
    "\n",
    "        completion_tokens = completion.usage.completion_tokens\n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        total_tokens = completion.usage.total_tokens\n",
    "\n",
    "        gpt_output = completion.choices[0].message.content\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        # Calculate hours, minutes, seconds, and milliseconds\n",
    "\n",
    "        minutes = int((elapsed_time % 3600) // 60)\n",
    "        seconds = int(elapsed_time % 60)\n",
    "        milliseconds = int((elapsed_time % 1) * 1000)\n",
    "\n",
    "        # Format the output as mm:ss.ms\n",
    "        formatted_time = f\"{minutes:02}:{seconds:02}.{milliseconds:03}\"\n",
    "\n",
    "        # Write the output of the prompting\n",
    "        write_output(\n",
    "            gpt_output=gpt_output,\n",
    "            user_prompt=user_prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            prompt_title=prompt_title,\n",
    "            directory=output_directory,\n",
    "            input_data_dict=input_data_dict,\n",
    "        )\n",
    "\n",
    "        # Write the prompt metadata\n",
    "        write_prompt_metadata(\n",
    "            completion_tokens=completion_tokens,\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            total_tokens=total_tokens,\n",
    "            elapsed_time=formatted_time,\n",
    "            directory=output_directory,\n",
    "            prompt_title=prompt_title,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Return zeros and the error message in case of an exception\n",
    "        print(str(e))\n",
    "        return 0, 0, 0, f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attribute_dict(attribute, column_name: str) -> pd.DataFrame:\n",
    "\n",
    "    attribute = pd.DataFrame(attribute)\n",
    "    attribute[\"original_index\"] = attribute.index\n",
    "\n",
    "    # Retreive unique values\n",
    "    attribute_unique = pd.DataFrame(\n",
    "        attribute[column_name].unique(), columns=[column_name]\n",
    "    )\n",
    "\n",
    "    # Create an index for unique values\n",
    "    attribute_unique[\"unique_index\"] = attribute_unique.index\n",
    "\n",
    "    # Join unique indexs to the original values, creating a dictionary of original index, value, and unique index\n",
    "    attribute_dict = pd.merge(\n",
    "        attribute, attribute_unique, on=column_name, how=\"left\", suffixes=(\"\", \"_df2\")\n",
    "    )\n",
    "\n",
    "    attribute_dict.rename(columns={\"index_df2\": \"unique_index\"}, inplace=True)\n",
    "\n",
    "    return attribute_dict, attribute_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"hospital\"\n",
    "\n",
    "dataset = pd.read_csv(f\"./datasets/{dataset}/dirty.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Level Prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"math_response\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"output\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"explanation\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The explanation for why a value is considered an error\",\n",
    "                            },\n",
    "                            \"index\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"The index in the list where the value occurs\",\n",
    "                            },\n",
    "                            \"annotation\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"The annotation denoting whether a value is an error or not\",\n",
    "                                \"enum\": [1, 0],\n",
    "                            },\n",
    "                            \"possible_repair\": {\"type\": \"string\"},\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"explanation\",\n",
    "                            \"index\",\n",
    "                            \"annotation\",\n",
    "                            \"possible_repair\",\n",
    "                        ],\n",
    "                        \"additionalProperties\": False,\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"output\"],\n",
    "            \"additionalProperties\": False,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_prompt = \"\"\"You are given a list of numeric values with their corresponding index. You have to identify all errors in the list.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompt = \"\"\"You are given a list of unique values in a column with their corresponding index. You have to find all syntactic errors in the dataset and recommend a possible repair.\n",
    "A syntactic error occurs when a value does not conform to the structure or domain of correct values. The domain and structure of correct values have to be derived from the values themselves.\n",
    "A semantic error occurs when a value falls outside of the reasonable context of a column. Use the context description to determine if a value is a semantic error.\n",
    "You have to annotate an error with a '1' and a correct value of the '0' in the output.\n",
    "For the possible repair only provide the reparied value is output.\n",
    "You also have to provide a brief explanation referencing the examples a proof for each annotation.\n",
    "Evaluate each value and provide an annotation and explanation regardless of error status.\n",
    "Values denoting empty or null values can be found in any given context, and are considered correct.\n",
    "Note! Only check for syntactic errors. Do not check for language errors.\n",
    "\n",
    "Syntactic deviations can be one of the following examples\n",
    "1. Invalid characters\n",
    "- Characters appear in values that do not often appear in others or make them uninterpretable\n",
    "Example 1\n",
    "1, John = 0\n",
    "2, Greg = 0\n",
    "3, Frank15 = 1\n",
    "\n",
    "Example 2 \n",
    "15, Apple = 0\n",
    "16, Pxar = 1\n",
    "17, Banana = 0\n",
    "\n",
    "2. Misspelling\n",
    "- Words that are misspelled. Values that are considered names are less likely to be misspelt.\n",
    "Example 1\n",
    "2, Blue = 0\n",
    "3, Green = 0\n",
    "4, Orage = 1\n",
    "\n",
    "Example 2 \n",
    "67, Reservation for two people near the window = 0\n",
    "68, Reservatton for five poeple at the entrance = 1\n",
    "69, Three humans arriving at 9 for drinks = 0\n",
    "70, A single peersonn at three = 1\n",
    "\n",
    "3. Pattern non-conformity\n",
    "- Some values may have a common pattern with certain values deviating from this pattern. There might exist more than one valid pattern in a single attribute.\n",
    "Possible repairs should attempt to conform with the most prevalent patterns. Removing a pattern does not constitute a soluition.\n",
    "Example 1\n",
    "32, 2024/03/12 19:00 = 0\n",
    "33, 2024-12-31 12:00 = 1\n",
    "34, 1994/01/13 15:15 = 0\n",
    "35, 12:00am 3 January 2024 = 1\n",
    "\n",
    "Example 2\n",
    "70, Admin123 = 0\n",
    "71, Bob443 = 0\n",
    "72, 99Alex = 1\n",
    "\"\"\"\n",
    "\n",
    "# And a context description of the attribute.\n",
    "# 4. Contextual meaning\n",
    "# - The contextual meaning of a column could provide some clues as to the expected values in a column.\n",
    "# Example 1\n",
    "# Contextual meaning: A column possibly storing state codes\n",
    "# 22, MA = 0\n",
    "# 23, TX = 0\n",
    "# 24, ZA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_attribute_prompt_string(attribute_unique) -> str:\n",
    "    attribute_delimited = attribute_unique[\n",
    "        [attribute_unique.columns[1], attribute_unique.columns[0]]\n",
    "    ]\n",
    "\n",
    "    if attribute_delimited[attribute_unique.columns[0]].dtype != \"int64\":\n",
    "        attribute_delimited.loc[:, attribute_unique.columns[0]] = attribute_delimited[\n",
    "            attribute_unique.columns[0]\n",
    "        ].apply(lambda x: x.strip())\n",
    "\n",
    "    attribute_delimited.loc[:, attribute_unique.columns[0]] = attribute_delimited[\n",
    "        attribute_unique.columns[0]\n",
    "    ].apply(lambda x: f\"|{x}|\")\n",
    "\n",
    "    attribute_delimited.loc[:, attribute_unique.columns[1]] = attribute_delimited[\n",
    "        attribute_unique.columns[1]\n",
    "    ].apply(lambda x: f\"[{x}]\")\n",
    "\n",
    "    attribute_unique_string = attribute_delimited.to_csv(\n",
    "        index=False, header=False, lineterminator=\"\\n\", sep=\">\"\n",
    "    )\n",
    "\n",
    "    return attribute_unique_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\index\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\provider_number\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\name\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\address_1\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\address_2\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\address_3\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\city\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\state\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\zip\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\county\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\phone\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\type\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\owner\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\emergency_service\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\condition\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\measure_code\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\measure_name\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\score\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\sample\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\\state_average\n",
      "Processing complete. Results and token/time data have been saved.\n"
     ]
    }
   ],
   "source": [
    "for col in dataset.columns:\n",
    "    attribute = pd.DataFrame(dataset[col].copy())\n",
    "\n",
    "    attribute_dict, attribute_unique = create_attribute_dict(attribute, col)\n",
    "\n",
    "    if attribute[attribute.columns[0]].dtype == object:\n",
    "        system_prompt = text_prompt\n",
    "    else:\n",
    "        system_prompt = numeric_prompt\n",
    "\n",
    "    attribute_unique.columns = [\"value\", \"index\"]\n",
    "    json_sample = attribute_unique.to_json(orient=\"records\", indent=4)\n",
    "\n",
    "    user_prompt = f\"\"\"Input:\n",
    "    {json_sample}\n",
    "\"\"\"\n",
    "    directory = r\"D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\"\n",
    "\n",
    "    prompt_gpt(\n",
    "        system_prompt,\n",
    "        user_prompt,\n",
    "        col,\n",
    "        response_format,\n",
    "        directory,\n",
    "        attribute_dict,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store data for each folder\n",
    "data_dict = {}\n",
    "\n",
    "# Loop through all subdirectories and files\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    # Get the folder name (which will be the column name)\n",
    "    folder_name = os.path.basename(root)\n",
    "\n",
    "    # Skip the root directory itself (attribute_output)\n",
    "    if root == directory:\n",
    "        continue\n",
    "\n",
    "    annotated_output = None\n",
    "    dict_data = None\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            # Open and load the JSON file\n",
    "            with open(file_path, \"r\") as json_file:\n",
    "                try:\n",
    "                    json_data = json.load(json_file)\n",
    "                    # Retrieve the annotation and index from the json output and create a dataframe\n",
    "                    annotations = [item[\"annotation\"] for item in json_data[\"output\"]]\n",
    "                    index = [item[\"index\"] for item in json_data[\"output\"]]\n",
    "\n",
    "                    annotated_output = pd.DataFrame(\n",
    "                        {\"annotation\": annotations, \"index\": index}\n",
    "                    )\n",
    "\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON from file {file_path}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "        elif file == \"dict.csv\":\n",
    "            dict_file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                dict_data = pd.read_csv(dict_file_path)\n",
    "                dict_data.columns = dict_data.columns.str.strip()\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading dict.csv from file {dict_file_path}: {e}\")\n",
    "\n",
    "    if annotated_output is not None and dict_data is not None:\n",
    "        annotated_output = annotated_output.drop_duplicates(\n",
    "            subset=\"index\", keep=\"first\"\n",
    "        )\n",
    "\n",
    "        dict_out_merged = pd.merge(\n",
    "            dict_data,\n",
    "            annotated_output,\n",
    "            left_on=\"unique_index\",\n",
    "            right_on=\"index\",\n",
    "            how=\"left\",\n",
    "        )  # Join unique indices to the original values\n",
    "\n",
    "        dict_out_merged.fillna(0, inplace=True)\n",
    "        data_dict[folder_name] = dict_out_merged[\"annotation\"]\n",
    "    else:\n",
    "        print(f\"Missing files for folder {folder_name}\")\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "output = pd.DataFrame(data_dict)\n",
    "output = output[dataset.columns.str.strip()]  # Strip whitespace from columns\n",
    "output = output.astype(int)\n",
    "output.to_csv(\"./output/attribute_output/output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_csv(\"./output/attribute_output/output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_annotation = pd.read_csv(\n",
    "    \"./datasets/hospital/errors.csv\", header=None, index_col=None, skiprows=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df1, df2):\n",
    "    # Flatten the dataframes to 1D arrays\n",
    "    y_true = df1.values.flatten()\n",
    "    y_pred = df2.values.flatten()\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Precision\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "\n",
    "    # Recall\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "\n",
    "    f_score = f1_score(y_true, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9793\n",
      "Precision: 0.8231827111984283\n",
      "Recall: 0.5639300134589502\n",
      "F1 score: 0.6693290734824281\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f_score = calculate_metrics(output, error_annotation)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 score: {f_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_classification(error_annotation, output, input):\n",
    "    error_annotation.columns = input.columns\n",
    "    output.columns = input.columns\n",
    "\n",
    "    calc = error_annotation.add(2)\n",
    "    calc_out = output\n",
    "    calc_out[calc_out == 0] = -1\n",
    "\n",
    "    calc = calc.add(calc_out)\n",
    "\n",
    "    # True positive calculation\n",
    "    tp = calc == 4\n",
    "    tp = input[tp]\n",
    "\n",
    "    # False positive calculation\n",
    "    fp = calc == 3\n",
    "    fp = input[fp]\n",
    "\n",
    "    # False negative calculation\n",
    "    fn = calc == 2\n",
    "    fn = input[fn]\n",
    "\n",
    "    return tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, fn = inspect_classification(\n",
    "    error_annotation=error_annotation,\n",
    "    output=output,\n",
    "    input=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.to_csv(\"./output/attribute_output/tp.csv\")\n",
    "fp.to_csv(\"./output/attribute_output/fp.csv\")\n",
    "fn.to_csv(\"./output/attribute_output/fn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_errors(\n",
    "    df_fixed: pd.DataFrame, df_with_errors: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    # Check if the dataframes have the same shape and columns after sorting\n",
    "    if df_fixed.shape != df_with_errors.shape or not all(\n",
    "        df_fixed.columns == df_with_errors.columns\n",
    "    ):\n",
    "        raise ValueError(\"Both dataframes must have the same structure.\")\n",
    "\n",
    "    # Convert both dataframes to strings for datatype-agnostic comparison\n",
    "    df_fixed_str = df_fixed.astype(str)\n",
    "    df_with_errors_str = df_with_errors.astype(str)\n",
    "\n",
    "    # Create the annotation dataframe by comparing the two dataframes\n",
    "    error_annotation = (df_fixed_str != df_with_errors_str).astype(int)\n",
    "\n",
    "    return error_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty = pd.read_csv(\"./datasets/hospital/dirty.csv\")\n",
    "clean = pd.read_csv(\"./datasets/hospital/clean.csv\")\n",
    "\n",
    "\n",
    "error_annotation = annotate_errors(clean, dirty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utill Here in Flask. Test to see if it works. Next, recreate the code to get the error annotation when you have a dirty dataset and a clean dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"math_response\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"output\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"columns\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\"type\": \"number\"},\n",
    "                                \"description\": \"An array of columns that might share a dependency.\",\n",
    "                            },\n",
    "                            \"dependency\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"A description of the dependency between the identified columns.\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"columns\", \"dependency\"],\n",
    "                        \"additionalProperties\": False,\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"output\"],\n",
    "            \"additionalProperties\": False,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are given a sample of similar records from a relational database. You have to determine which columns might have dependencies between them. \n",
    "For the output, only provide pairs of columns that might have dependencies. If multipule dependencies exist with a single column, provide more than one outputs for that column.\n",
    "Do not check for dependencies between a column and itself.\n",
    "\n",
    "Dependencies can occur in two ways.\n",
    "\n",
    "1. Semantic dependency\n",
    "- The meaning of the values of one column determines the meaning of another. \n",
    "Example:\n",
    "One column represents cities, and another countries. This may indicated that there is a dependency between the two columns, and that the citities column, contains cities that are present in that country.\n",
    "Col 1                   , Col 2\n",
    "Great Britain           , London\n",
    "United States of America, Washington DC\n",
    "South Africa            , Pretoria\n",
    "\n",
    "Note! Other semantic dependecies may occur in different domains from the one mentioned in the example.\n",
    "\n",
    "2. Pattern Dependency\n",
    "- One column may have a pattern, which in part is based on the meaning of another column. \n",
    "Example:\n",
    "One column represents emergency codes, the other emergency descriptions.\n",
    "Col 1      , Col 2\n",
    "FRE-003_a  , Forrest fire with damages above $2,000\n",
    "FLD-001_z  , Floods that destroyed local buildings\n",
    "ERQ-777_n  , Earthquakes that caused power outages \n",
    "HUR-008_t  , Hurricanes that was able to breach sea walls\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = dataset.values.tolist()\n",
    "\n",
    "# Create an LSH index with a threshold and number of permutations\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "\n",
    "# Insert each record into the LSH index\n",
    "for i, record in enumerate(records):\n",
    "    m = MinHash(num_perm=128)\n",
    "    for feature in record:\n",
    "        m.update(str(feature).encode(\"utf8\"))  # Hashing the attributes of the record\n",
    "    lsh.insert(i, m)\n",
    "\n",
    "    buckets = []\n",
    "visited = set()  # To track records that have already been assigned to a bucket\n",
    "\n",
    "# Querying similar records for each record\n",
    "for i, record in enumerate(records):\n",
    "    if i not in visited:  # Only process records that haven't been visited\n",
    "        # Create MinHash for the current record\n",
    "        m = MinHash(num_perm=128)\n",
    "        for feature in record:\n",
    "            m.update(str(feature).encode(\"utf8\"))\n",
    "\n",
    "        # Query LSH to get similar records\n",
    "        similar_records = lsh.query(m)\n",
    "\n",
    "        # Add the current record and its similar ones as a new bucket\n",
    "        buckets.append(similar_records)\n",
    "\n",
    "        # Mark all similar records as visited\n",
    "        visited.update(similar_records)\n",
    "\n",
    "# Calculate the sizes of all buckets\n",
    "bucket_sizes = [(i, len(bucket)) for i, bucket in enumerate(buckets)]\n",
    "\n",
    "# Sort buckets by size in descending order\n",
    "sorted_buckets = sorted(bucket_sizes, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Retrieve the top 10 buckets\n",
    "top_buckets_raw = sorted_buckets[:10]  # Get the top 10 buckets\n",
    "\n",
    "# Define a minimum size threshold (e.g., 2)\n",
    "min_size_threshold = 2\n",
    "\n",
    "# Filter the top buckets based on the minimum size threshold\n",
    "filtered_top_buckets = [\n",
    "    bucket for bucket in top_buckets_raw if bucket[1] >= min_size_threshold\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency\\bucket_29\\output\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency\\bucket_2\\output\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency\\bucket_0\\output\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency\\bucket_173\\output\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency\\bucket_225\\output\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency\\bucket_23\\output\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency\\bucket_53\\output\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency\\bucket_155\\output\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency\\bucket_230\\output\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency\\bucket_1\\output\n",
      "Processing complete. Results and token/time data have been saved.\n"
     ]
    }
   ],
   "source": [
    "for bucket_index, _ in filtered_top_buckets:\n",
    "    dynamic_directory = os.path.join(\n",
    "        r\"D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency\",\n",
    "        f\"bucket_{bucket_index}\",\n",
    "    )\n",
    "\n",
    "    os.makedirs(dynamic_directory, exist_ok=True)\n",
    "\n",
    "    current_bucket_records = [records[i] for i in buckets[bucket_index]]\n",
    "\n",
    "    dataset_sample = pd.DataFrame(current_bucket_records)\n",
    "    dataset_sample.columns = range(dataset_sample.shape[1])\n",
    "\n",
    "    json_sample = dataset_sample.to_json(orient=\"records\", indent=4)\n",
    "\n",
    "    user_prompt = f\"\"\"Input:\n",
    "The following is a formatted table with the data to be checked.\n",
    "{json_sample}\n",
    "\"\"\"\n",
    "\n",
    "    # Call your prompt_gpt function with the dynamic directory\n",
    "    prompt_gpt(\n",
    "        system_prompt,\n",
    "        user_prompt,\n",
    "        \"output\",\n",
    "        response_format,\n",
    "        dynamic_directory,\n",
    "        dataset_sample,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     columns                                         dependency\n",
      "0     [0, 1]  Unique record identifiers may have an associat...\n",
      "1     [1, 2]  The zip code (column 1) indicates the specific...\n",
      "2     [1, 6]  The zip code (column 1) suggests that the loca...\n",
      "3    [1, 11]  The facility ID (Col 1) is related to the cate...\n",
      "4    [1, 12]  The facility ID (Col 1) is related to the type...\n",
      "5    [1, 14]  The facility ID (Col 1) determines the type of...\n",
      "6    [1, 19]  The identifier (column 19) corresponds context...\n",
      "7     [2, 3]  The street address (Col 3) likely relies on th...\n",
      "8     [2, 9]  The hospital name (Col 2) might relate to the ...\n",
      "9    [2, 12]  The hospital name (Col 2) is dependent on the ...\n",
      "10   [2, 14]  The medical center name in column 2 indicates ...\n",
      "11    [3, 4]  The address (Col 4) likely depends on the stre...\n",
      "12    [4, 9]  The county name (Col 9) may semantically depen...\n",
      "13   [5, 14]  The disease category (Col 14) might depend on ...\n",
      "14    [6, 7]  The city (Col 6) relates to the state (Col 7) ...\n",
      "15    [6, 8]  The city (Col 6) relates semantically to the z...\n",
      "16    [7, 8]  The state abbreviation (al) matches the zip co...\n",
      "17    [8, 9]  The area code (column 8) relates to the specif...\n",
      "18  [10, 11]  The phone number (Col 10) relates to the hospi...\n",
      "19  [11, 14]  The type of hospital corresponds to specific c...\n",
      "20  [12, 13]  The type of hospital district/authority may de...\n",
      "21  [12, 14]  The hospital type (Col 12) could have a relati...\n",
      "22  [12, 19]  The type of hospital district may relate to th...\n",
      "23  [13, 14]  The consent status (column 13) influences the ...\n",
      "24  [13, 19]  The measure identifier (column 19) may correla...\n",
      "25  [14, 15]  Pattern dependency where column 15 refers to h...\n",
      "26  [14, 16]  The description of care for each entry is rela...\n",
      "27  [14, 17]  The percentage of care provided is linked to t...\n",
      "28  [14, 19]  The heart condition (column 14) ties to a spec...\n",
      "29  [15, 16]  The identifier in column 15 corresponds to the...\n",
      "30  [15, 17]  The performance measure (percentage) relates t...\n",
      "31  [15, 18]  The number of patients in column 18 is related...\n",
      "32  [15, 19]  The identifier in column 15 patterns the forma...\n",
      "33  [16, 17]  The description of patient care in column 16 c...\n",
      "34  [16, 19]  The detailed description of care in column 16 ...\n",
      "35  [17, 18]  The recovery percentage (Col 17) is potentiall...\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to hold unique dependencies\n",
    "dependencies_dict = defaultdict(set)\n",
    "\n",
    "# Assuming you have a list of dynamic directories for each bucket\n",
    "for bucket_index, _ in filtered_top_buckets:\n",
    "    # Construct the directory for the current bucket\n",
    "    dynamic_directory = os.path.join(\n",
    "        r\"D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency\",\n",
    "        f\"bucket_{bucket_index}\\output\\output.json\",\n",
    "    )\n",
    "\n",
    "    # Check if the JSON file exists before trying to read it\n",
    "    if os.path.exists(dynamic_directory):\n",
    "        with open(dynamic_directory, \"r\") as json_file:\n",
    "            try:\n",
    "                json_data = json.load(json_file)\n",
    "\n",
    "                # Retrieve the annotations and create a list of tuples for dependencies\n",
    "                for item in json_data[\"output\"]:\n",
    "                    columns = item[\"columns\"]\n",
    "                    # Sort the columns numerically to ensure the order is consistent\n",
    "                    sorted_columns_tuple = tuple(\n",
    "                        sorted(columns, key=int)\n",
    "                    )  # Sort numerically\n",
    "                    dependency_description = item[\"dependency\"]\n",
    "\n",
    "                    # Store the dependency description in the set for this tuple of columns\n",
    "                    dependencies_dict[sorted_columns_tuple].add(dependency_description)\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from file {dynamic_directory}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {dynamic_directory}: {e}\")\n",
    "\n",
    "# Prepare the final list for DataFrame\n",
    "dependencies_list = []\n",
    "for columns, descriptions in dependencies_dict.items():\n",
    "\n",
    "    first_description = next(iter(descriptions))  # Extract the first item from the set\n",
    "    dependencies_list.append(\n",
    "        {\n",
    "            \"columns\": list(columns),  # Convert back to list for the DataFrame\n",
    "            \"dependency\": first_description,  # Store only the first description\n",
    "            # \"dependency\": list(descriptions),  # Store all descriptions as a list\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Sort dependencies list first by the first column, then by the second column\n",
    "dependencies_list.sort(key=lambda x: (x[\"columns\"][0], x[\"columns\"][1]))\n",
    "\n",
    "# Create a DataFrame from the sorted unique dependencies\n",
    "dependencies_df = pd.DataFrame(dependencies_list)\n",
    "\n",
    "# Optionally, you can display the DataFrame or save it to a file\n",
    "print(dependencies_df)\n",
    "\n",
    "# Save the sorted dependencies DataFrame to a CSV file\n",
    "dependencies_df.to_csv(\"dependencies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"math_response\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"output\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"explanation\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The explanation for why a value is considered an error\",\n",
    "                            },\n",
    "                            \"index\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"The index in the list where the value occurs\",\n",
    "                            },\n",
    "                            \"column\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"The column where the violation occured\",\n",
    "                            },\n",
    "                            \"annotation\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"The annotation denoting whether a value is an error or not\",\n",
    "                                \"enum\": [1, 0],\n",
    "                            },\n",
    "                            \"possible_repair\": {\"type\": \"string\"},\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"explanation\",\n",
    "                            \"index\",\n",
    "                            \"annotation\",\n",
    "                            \"possible_repair\",\n",
    "                            \"column\",\n",
    "                        ],\n",
    "                        \"additionalProperties\": False,\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"output\"],\n",
    "            \"additionalProperties\": False,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are given data from columns in a dataset that is said to have a dependency between each other. You have to detect violations in this dependency.\n",
    "\n",
    "You are also given a description of the dependency that you can use to help you identify violations.\n",
    "\n",
    "For the possible repair provide only the repair value. The error value and column is the value that is abnormal and that causes the violation.\n",
    "\n",
    "Dependecy violations could be either semantic or syntactic violations. Use the dependency description to determine the violation. Do not check for language usage errors.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dependency_prompt_string(columns: pd.DataFrame) -> str:\n",
    "    # Create a true copy of the DataFrame to avoid warnings about modifying slices\n",
    "    data_columns = columns.copy()\n",
    "\n",
    "    # Remove duplicate rows based on all columns\n",
    "    unique_combinations = data_columns.drop_duplicates(\n",
    "        subset=data_columns.columns.tolist()\n",
    "    ).copy()\n",
    "\n",
    "    # Add index as a new column, and format it with brackets using .loc[]\n",
    "    unique_combinations[\"index\"] = (\n",
    "        unique_combinations.index\n",
    "    )  # Ensure you're adding a new column\n",
    "    unique_combinations[\"index\"] = unique_combinations[\"index\"].apply(\n",
    "        lambda x: f\"[{x}]\"\n",
    "    )\n",
    "\n",
    "    # Apply delimiters to other columns\n",
    "    for col in unique_combinations.columns:\n",
    "        if col != \"index\":  # Skip the 'index' column\n",
    "            unique_combinations.loc[:, col] = unique_combinations[col].apply(\n",
    "                lambda x: f\"|{x}|\"\n",
    "            )\n",
    "\n",
    "    # Reorder columns to move 'index' to the front\n",
    "    cols = [\"index\"] + [col for col in unique_combinations.columns if col != \"index\"]\n",
    "    unique_combinations = unique_combinations[cols]\n",
    "\n",
    "    # Convert the DataFrame to CSV format with the required delimiters\n",
    "    unique_combinations_string = unique_combinations.to_csv(\n",
    "        index=False, header=False, lineterminator=\"\\n\", sep=\",\"\n",
    "    )\n",
    "\n",
    "    return unique_combinations_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create unique row dictionary\n",
    "def create_row_dict(selected_columns: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Make a copy to avoid SettingWithCopyWarning\n",
    "    selected_columns = selected_columns.copy()\n",
    "\n",
    "    # Store original index\n",
    "    selected_columns[\"original_index\"] = selected_columns.index\n",
    "    # Retrieve unique rows by dropping duplicates, ignoring 'original_index'\n",
    "    unique_rows = (\n",
    "        selected_columns.drop(\"original_index\", axis=1)\n",
    "        .drop_duplicates(subset=selected_columns.columns[:-1])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    unique_rows[\"unique_index\"] = (\n",
    "        unique_rows.index\n",
    "    )  # Create unique index for unique rows\n",
    "\n",
    "    # print(selected_columns.columns)\n",
    "    # print(unique_rows.columns)\n",
    "\n",
    "    # Merge unique row index back to original data to track the mapping\n",
    "    row_dict = pd.merge(\n",
    "        selected_columns,\n",
    "        unique_rows,\n",
    "        how=\"left\",\n",
    "        on=list(\n",
    "            selected_columns.columns[:-1]\n",
    "        ),  # Merge on actual data columns, excluding 'original_index' and 'unique_row_index'\n",
    "    )\n",
    "    # attribute_dict = pd.merge(\n",
    "    #     attribute, attribute_unique, on=column_name, how=\"left\", suffixes=(\"\", \"_df2\")\n",
    "    # )\n",
    "    # (print(selected_columns.columns[:-1]))\n",
    "    # (print(row_dict))\n",
    "\n",
    "    return row_dict, unique_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[1, 2]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[1, 6]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[1, 11]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[1, 12]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[1, 14]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[2, 3]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[2, 9]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[2, 12]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[2, 14]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[3, 4]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[4, 9]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[5, 14]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[6, 7]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[6, 8]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[7, 8]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[8, 9]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[10, 11]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[11, 14]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[12, 13]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[12, 14]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[12, 19]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[13, 14]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[13, 19]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[14, 15]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[14, 16]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[14, 17]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[14, 19]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[15, 16]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[15, 17]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[15, 18]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[15, 19]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[16, 17]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[16, 19]\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\[17, 18]\n",
      "Processing complete. Results and token/time data have been saved.\n"
     ]
    }
   ],
   "source": [
    "# Group the dependencies by the first index\n",
    "grouped_dependencies = defaultdict(list)\n",
    "\n",
    "for _, row in dependencies_df.iterrows():\n",
    "    first_index = row[\"columns\"][0]  # Get the first index of the pair\n",
    "    grouped_dependencies[first_index].append(row)  # Group by first index\n",
    "\n",
    "# Now process each group of dependencies based on the first index\n",
    "for first_index, dependencies in grouped_dependencies.items():\n",
    "\n",
    "    selected_columns = [\n",
    "        dataset.columns[first_index]\n",
    "    ]  # Select columns based on the first index\n",
    "    dependencies_list = []\n",
    "\n",
    "    for dep in dependencies:\n",
    "        columns = dep[\"columns\"]\n",
    "        dependency_description = dep[\"dependency\"]\n",
    "        selected_columns = dataset[\n",
    "            dataset.columns[columns]\n",
    "        ]  # Get the relevant columns from the dataset\n",
    "\n",
    "        # Check if there are duplicates\n",
    "        if selected_columns.drop_duplicates().shape[0] < dataset.shape[0]:\n",
    "            # Create the unique row dictionary using the previously defined create_row_dict function\n",
    "            row_dict, unique_rows = create_row_dict(selected_columns)\n",
    "\n",
    "            # Only use the unique rows for prompting\n",
    "            directory = r\"D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\"\n",
    "\n",
    "            # Prepare the data to prompt for each unique row combination\n",
    "            # data_columns = unique_rows.drop(\"original_index\", axis=1)\n",
    "\n",
    "            unique_rows.columns = [\n",
    "                str(columns[0]),\n",
    "                str(columns[1]),\n",
    "                \"index\",\n",
    "            ]\n",
    "\n",
    "            json_sample = unique_rows.to_json(orient=\"records\", indent=4)\n",
    "\n",
    "            # Prepare the user prompt with dependency and unique rows\n",
    "            user_prompt = f\"\"\"Input:\n",
    "The dependency identified in this table is defined as follows:\n",
    "{dependency_description}\n",
    "\n",
    "The following is a formatted table with the unique data to be checked.\n",
    "{json_sample}\n",
    "\"\"\"\n",
    "\n",
    "            # Uncomment this to send the prompt to the GPT system\n",
    "            prompt_gpt(\n",
    "                system_prompt,\n",
    "                user_prompt,\n",
    "                str(columns),\n",
    "                response_format,\n",
    "                directory,\n",
    "                row_dict,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the directory containing the JSON files\n",
    "directory = r\"D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\"\n",
    "\n",
    "# Assuming the original DataFrame structure is known\n",
    "original_dataframe = dataset  # Replace with your actual DataFrame\n",
    "\n",
    "# Initialize the DataFrame with zeros, same shape as original dataframe\n",
    "annotated_output = pd.DataFrame(\n",
    "    0, index=original_dataframe.index, columns=original_dataframe.columns\n",
    ")\n",
    "\n",
    "# Loop through all subdirectories and files\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    # Get the folder name (which could be the column name or some identifier)\n",
    "    folder_name = os.path.basename(root)\n",
    "\n",
    "    # Skip the root directory itself (attribute_output)\n",
    "    if root == directory:\n",
    "        continue\n",
    "\n",
    "    # Variables to store JSON and dictionary data\n",
    "    annotations_data = None\n",
    "    dict_data = None\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            # Construct the full file path for JSON\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            # Open and load the JSON file\n",
    "            with open(file_path, \"r\") as json_file:\n",
    "                try:\n",
    "                    json_data = json.load(json_file)\n",
    "\n",
    "                    # Retrieve the annotation, index, and column from the JSON output\n",
    "                    annotations = [item[\"annotation\"] for item in json_data[\"output\"]]\n",
    "                    indices = [item[\"index\"] for item in json_data[\"output\"]]\n",
    "                    columns = [item[\"column\"] for item in json_data[\"output\"]]\n",
    "\n",
    "                    # Create a DataFrame for the JSON annotations\n",
    "                    annotations_data = pd.DataFrame(\n",
    "                        {\"annotation\": annotations, \"index\": indices, \"column\": columns}\n",
    "                    )\n",
    "\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON from file {file_path}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "        elif file == \"dict.csv\":\n",
    "            # Construct the full file path for dict.csv\n",
    "            dict_file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                # Read the dict.csv file into a DataFrame\n",
    "                dict_data = pd.read_csv(dict_file_path)\n",
    "                dict_data.columns = dict_data.columns.str.strip()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading dict.csv from file {dict_file_path}: {e}\")\n",
    "\n",
    "    # Ensure both annotation data and dictionary data are available\n",
    "    if annotations_data is not None and dict_data is not None:\n",
    "        # Merge the annotation data (based on the index) with the dictionary data (unique_index mapping)\n",
    "        dict_out_merged = pd.merge(\n",
    "            dict_data,\n",
    "            annotations_data,\n",
    "            left_on=\"unique_index\",  # Use the unique index from dict.csv\n",
    "            right_on=\"index\",  # Match with index in the annotation data\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "        # Replace NaN annotations with 0\n",
    "        dict_out_merged.fillna(0, inplace=True)\n",
    "\n",
    "        # Process the detailed annotation matching to the original dataframe\n",
    "        for _, row in dict_out_merged.iterrows():\n",
    "            original_index = int(\n",
    "                row[\"original_index\"]\n",
    "            )  # Ensure original index is an integer\n",
    "            annotation = row[\"annotation\"]\n",
    "            column = int(row[\"column\"])  # Ensure the column is integer\n",
    "\n",
    "            if annotation == 1:  # Only proceed if there's an error annotation\n",
    "                # Update the annotated_output DataFrame at the specific cell\n",
    "                col_name = original_dataframe.columns[\n",
    "                    column\n",
    "                ]  # Get column name based on column index\n",
    "                annotated_output.at[original_index, col_name] = annotation\n",
    "\n",
    "    else:\n",
    "        print(f\"Missing files for folder {folder_name}\")\n",
    "\n",
    "# Save the annotated DataFrame to CSV\n",
    "annotated_output.to_csv(\"./output/dependency_violations/output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_output = pd.read_csv(\"./output/dependency_violations/output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86035\n",
      "Precision: 0.6208251473477406\n",
      "Recall: 0.1083676268861454\n",
      "F1 score: 0.18452554744525546\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f_score = calculate_metrics(\n",
    "    annotated_output, error_annotation\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 score: {f_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, fn = inspect_classification(\n",
    "    error_annotation=error_annotation,\n",
    "    output=annotated_output,\n",
    "    input=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.to_csv(\"./output/dependency_violations/tp.csv\")\n",
    "fp.to_csv(\"./output/dependency_violations/fp.csv\")\n",
    "fn.to_csv(\"./output/dependency_violations/fn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two datasets\n",
    "dataset_1 = pd.read_csv(\n",
    "    \"./output/dependency_violations/output.csv\"\n",
    ")  # Path to first dataset\n",
    "dataset_2 = pd.read_csv(\n",
    "    \"./output/attribute_output/output.csv\"\n",
    ")  # Path to second dataset\n",
    "\n",
    "# Ensure both datasets have the same structure (columns and index)\n",
    "# Initialize a consolidated DataFrame with the same shape as the original datasets\n",
    "consolidated_data = pd.DataFrame(0, index=dataset_1.index, columns=dataset_1.columns)\n",
    "\n",
    "# Loop through the indices and columns to consolidate annotations in place\n",
    "for col in dataset_1.columns:\n",
    "    for index in dataset_1.index:\n",
    "        # Check if the current index has an error in dataset 1\n",
    "        error_1 = dataset_1.at[index, col]\n",
    "        error_2 = (\n",
    "            dataset_2.at[index, col] if index < len(dataset_2) else 0\n",
    "        )  # Handle cases where index exceeds\n",
    "\n",
    "        # Logic to consolidate annotations\n",
    "        if error_1 == 1 or error_2 == 1:  # If either dataset has an error\n",
    "            consolidated_data.at[index, col] = 1  # Mark as error\n",
    "        else:\n",
    "            consolidated_data.at[index, col] = 0  # No error\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "consolidated_data.to_csv(\"./output/consolidated_error_annotations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8515\n",
      "Precision: 0.899803536345776\n",
      "Recall: 0.13562333432040272\n",
      "F1 score: 0.2357179619145651\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f_score = calculate_metrics(\n",
    "    consolidated_data, error_annotation\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 score: {f_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, fn = inspect_classification(\n",
    "    error_annotation=error_annotation,\n",
    "    output=output,\n",
    "    input=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.to_csv(\"./output/tp.csv\")\n",
    "fp.to_csv(\"./output/fp.csv\")\n",
    "fn.to_csv(\"./output/fn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "balanced accuracy\n",
    "comparing with only errors (accuracy)\n",
    "\n",
    "entity resolution for dependency preprocessing, clustering, blocking\\*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raha, synodc = experminets, experminental setup. Datasets, performance, baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for new papers on LLM data cleaning\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
