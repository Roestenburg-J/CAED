{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import time\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from typing import Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT API setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    organization=\"org-Efj3WwiBs01tiD9ogyAb1vgz\",\n",
    "    project=\"proj_ItFIKb0eOHXEFM65qPMVLpHt\",\n",
    "    api_key=\"sk-proj-waDJ9nwjNNOcQa6Ol0epT3BlbkFJbwwL9qZnqZmBMVCwtvOX\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompting Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the prompts used, and the GPT output to a folder for later inspection\n",
    "def write_output(\n",
    "    gpt_output: Dict[str, Any],\n",
    "    user_prompt: str,\n",
    "    system_prompt: str,\n",
    "    prompt_title: str,\n",
    "    directory: str,\n",
    "    input_data_dict: pd.DataFrame,\n",
    ") -> None:\n",
    "\n",
    "    try:\n",
    "        # Create the subdirectory for the pompt_title if it doesn't exist\n",
    "        prompt_directory = os.path.join(\n",
    "            directory, prompt_title.strip()\n",
    "        )  # Strip whitespace\n",
    "        os.makedirs(prompt_directory, exist_ok=True)\n",
    "\n",
    "        # Define the file paths\n",
    "        output_text_file = os.path.join(prompt_directory, \"output.json\")\n",
    "        user_prompt_text_file = os.path.join(prompt_directory, \"user_prompt.txt\")\n",
    "        sys_prompt_text_file = os.path.join(prompt_directory, \"system_prompt.txt\")\n",
    "        input_data_dict_file = os.path.join(prompt_directory, \"dict.csv\")\n",
    "\n",
    "        # Write the output\n",
    "        with open(output_text_file, \"w\") as f:\n",
    "            f.write(gpt_output)\n",
    "\n",
    "        # Write the prompt\n",
    "        with open(user_prompt_text_file, \"w\") as f:\n",
    "            f.write(user_prompt)\n",
    "\n",
    "        with open(sys_prompt_text_file, \"w\") as f:\n",
    "            f.write(system_prompt)\n",
    "\n",
    "        input_data_dict.to_csv(\n",
    "            input_data_dict_file, index=False, sep=\",\", lineterminator=\"\\n\"\n",
    "        )\n",
    "\n",
    "        print(f\"Output and prompt saved in directory: {prompt_directory}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the number of tokens usd, the time to retrieve the output and other metadata to a CSV\n",
    "def write_prompt_metadata(\n",
    "    completion_tokens: int,\n",
    "    prompt_tokens: int,\n",
    "    total_tokens: int,\n",
    "    elapsed_time: str,\n",
    "    directory: str,\n",
    "    prompt_title: str,\n",
    ") -> None:\n",
    "    current_data = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"Completion Tokens\": completion_tokens,\n",
    "                \"Prompt Tokens\": prompt_tokens,\n",
    "                \"Total Tokens\": total_tokens,\n",
    "                \"Elapsed Time\": elapsed_time,\n",
    "                \"Prompt Name\": prompt_title,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prompt_metadata_path = os.path.join(directory, \"prompt_metadata.csv\")\n",
    "\n",
    "    # Check if the file exists to determine whether to write the header\n",
    "    if os.path.exists(prompt_metadata_path):\n",
    "        current_data.to_csv(prompt_metadata_path, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        current_data.to_csv(prompt_metadata_path, mode=\"w\", header=True, index=False)\n",
    "\n",
    "    print(\"Processing complete. Results and token/time data have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pompt of the GPT, providing one system prompt, a user prompt, and the expected schema for the output\n",
    "def prompt_gpt(\n",
    "    system_prompt: str,\n",
    "    user_prompt: str,\n",
    "    prompt_title: str,\n",
    "    response_format: dict[str],\n",
    "    output_directory: str,\n",
    "    input_data_dict: pd.DataFrame,\n",
    ") -> None:\n",
    "\n",
    "    class Output(BaseModel):\n",
    "        output: list[str]\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            max_tokens=16000,\n",
    "            response_format=response_format,\n",
    "        )\n",
    "\n",
    "        completion_tokens = completion.usage.completion_tokens\n",
    "        prompt_tokens = completion.usage.prompt_tokens\n",
    "        total_tokens = completion.usage.total_tokens\n",
    "\n",
    "        gpt_output = completion.choices[0].message.content\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        # Calculate hours, minutes, seconds, and milliseconds\n",
    "\n",
    "        minutes = int((elapsed_time % 3600) // 60)\n",
    "        seconds = int(elapsed_time % 60)\n",
    "        milliseconds = int((elapsed_time % 1) * 1000)\n",
    "\n",
    "        # Format the output as mm:ss.ms\n",
    "        formatted_time = f\"{minutes:02}:{seconds:02}.{milliseconds:03}\"\n",
    "\n",
    "        # Write the output of the prompting\n",
    "        write_output(\n",
    "            gpt_output=gpt_output,\n",
    "            user_prompt=user_prompt,\n",
    "            system_prompt=system_prompt,\n",
    "            prompt_title=prompt_title,\n",
    "            directory=output_directory,\n",
    "            input_data_dict=input_data_dict,\n",
    "        )\n",
    "\n",
    "        # Write the prompt metadata\n",
    "        write_prompt_metadata(\n",
    "            completion_tokens=completion_tokens,\n",
    "            prompt_tokens=prompt_tokens,\n",
    "            total_tokens=total_tokens,\n",
    "            elapsed_time=formatted_time,\n",
    "            directory=output_directory,\n",
    "            prompt_title=prompt_title,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Return zeros and the error message in case of an exception\n",
    "        print(str(e))\n",
    "        return 0, 0, 0, f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attribute_dict(attribute, column_name: str) -> pd.DataFrame:\n",
    "\n",
    "    attribute = pd.DataFrame(attribute)\n",
    "    attribute[\"original_index\"] = attribute.index\n",
    "\n",
    "    # Retreive unique values\n",
    "    attribute_unique = pd.DataFrame(\n",
    "        attribute[column_name].unique(), columns=[column_name]\n",
    "    )\n",
    "\n",
    "    # Create an index for unique values\n",
    "    attribute_unique[\"unique_index\"] = attribute_unique.index\n",
    "\n",
    "    # Join unique indexs to the original values, creating a dictionary of original index, value, and unique index\n",
    "    attribute_dict = pd.merge(\n",
    "        attribute, attribute_unique, on=column_name, how=\"left\", suffixes=(\"\", \"_df2\")\n",
    "    )\n",
    "\n",
    "    attribute_dict.rename(columns={\"index_df2\": \"unique_index\"}, inplace=True)\n",
    "\n",
    "    return attribute_dict, attribute_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"hospital\"\n",
    "\n",
    "dataset = pd.read_csv(f\"./datasets/{dataset}/dirty.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deriving Context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt response format\n",
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"math_response\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"output\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"datatype\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"A description of the datatype of an attribute\",\n",
    "                            },\n",
    "                            \"range\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"A description of the range of the values in an attribute, and how they inform the context\",\n",
    "                            },\n",
    "                            \"meaning\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The inherit meaning that a column has based on its values\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"datatype\", \"range\", \"meaning\"],\n",
    "                        \"additionalProperties\": False,\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"output\"],\n",
    "            \"additionalProperties\": False,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_context = \"\"\"You are given a few example values from a column in a relational dataset. You have to inspect the values and derive their context in the form of a description for each column. \n",
    "The values themselves may contain valuable clues to help derive the context. Check for the following clues:\n",
    "\n",
    "1. Datatype\n",
    "- The datatype of a column could help give insight into how the values might be used or interpreted. \n",
    "Example:\n",
    "- Columns with decimal values rounded to two poistions could indicate that the column stores currency values.\n",
    "- Columns with a specific pattern, like a date format, could indicate a date of a certain kind. Other patterns could exist with hidden meaning, like an employee number that contains the employees birthdate.\n",
    "- Text values could represent different meaning, like the names of people or places, or be the nouns of objects. Such columns could also be descriptions of some kind.\n",
    "\n",
    "2. Range of values.\n",
    "- The range of values could give an indication of the context of the attribute.\n",
    "Example: \n",
    "- Values that range between 2.06 and 1.5 could indicate the length of a person. So could values between -50 and +50 indicated degrees celcuis. Many other contexts may exist that is given by the range of numeric values.\n",
    "- Dates between 1930 and 2024 might indicated birthdates. Dates with a smaller spread could represent transactions or deliveries.\n",
    "- Text values with long strings are more likely to be descriptions than names or nouns.\n",
    "\n",
    "3. Inherit meaning\n",
    "- In some cases the values themselves might give a clear indication of their meaning.\n",
    "Example:\n",
    "- Text values like John, Sarah and Marcy indicates that the column is used for first names. Values like New York, London, and Dubai, Indicate that the columns are used for city names.\n",
    "- Some numeric values might include unit measurement symbols, like oz or km. Combining this information with the type of value and range could inform on the context of the attribute.\n",
    "- Inherit meaning is most likely to be observed in text columns.\n",
    "- Some columns might have hidden meaning in only part of the values. For example, the first three letters of US social security numbers indicate the area where the code was issued, and could indicate a hidden dependecy with a column like area code or area name. Always check non natural lanuage text fields for such patterns.\n",
    "\n",
    "Instructions:\n",
    "For each set of values:\n",
    "1. Describe the datatype.\n",
    "2. Summarize the range of the values.\n",
    "3. Identify inherit meaning if possible.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_attribute_string(attribute_unique) -> str:\n",
    "    if attribute_unique[attribute_unique.columns[0]].dtype != \"int64\":\n",
    "        attribute_unique[attribute_unique.columns[0]] = attribute_unique[\n",
    "            attribute_unique.columns[0]\n",
    "        ].apply(lambda x: x.strip())\n",
    "        attribute_unique[attribute_unique.columns[0]] = attribute_unique[\n",
    "            attribute_unique.columns[0]\n",
    "        ].apply(lambda x: f\"|{x}|\")\n",
    "    attribute_unique_string = attribute_unique.to_csv(\n",
    "        index=False, header=False, lineterminator=\"\\n\"\n",
    "    )\n",
    "\n",
    "    return attribute_unique_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\index\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\provider_number\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\name\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\address_1\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\address_2\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\address_3\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\city\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\state\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\zip\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\county\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\phone\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\type\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\owner\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\emergency_service\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\condition\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\measure_code\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\measure_name\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\score\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\sample\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\\state_average\n",
      "Processing complete. Results and token/time data have been saved.\n"
     ]
    }
   ],
   "source": [
    "# For each column in the dataset derive the context while prompting the GPT\n",
    "for col in dataset.columns:\n",
    "\n",
    "    attribute = dataset[col].copy()\n",
    "    attribute_dict, attribute_unique = create_attribute_dict(attribute, col)\n",
    "\n",
    "    directory = (\n",
    "        r\"D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\"\n",
    "    )\n",
    "\n",
    "    attribute.to_csv(index=False, header=True, lineterminator=\"\\n\", sep=\",\")\n",
    "\n",
    "    user_prompt = f\"\"\"Input:\n",
    "The following is a list of unique values in an attribute. \n",
    "Check the values and derive the context as indicated. The attribute values is delimited with \"|value|\".\n",
    "{generate_context_attribute_string(attribute_unique)}\n",
    "\"\"\"\n",
    "\n",
    "    prompt_gpt(\n",
    "        system_prompt_context, user_prompt, col, response_format, directory, attribute\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = (\n",
    "    r\"D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\context\"\n",
    ")\n",
    "\n",
    "# Initialize an empty dictionary to store data for each folder\n",
    "data_dict = {}\n",
    "\n",
    "# Loop through all subdirectories and files\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        # Get the folder name (which will be the column name)\n",
    "        folder_name = os.path.basename(root)\n",
    "        if file.endswith(\".json\"):\n",
    "            # Construct the full file path\n",
    "\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            # Open and load the JSON file\n",
    "            with open(file_path, \"r\") as json_file:\n",
    "                try:\n",
    "                    json_data = json.load(json_file)\n",
    "\n",
    "                    # Retrieve the context and join the list into a single string\n",
    "                    context = \"; \".join(\n",
    "                        [item[\"meaning\"] for item in json_data[\"output\"]]\n",
    "                    )\n",
    "                    data_dict[folder_name] = context\n",
    "\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON from file {file_path}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "output = pd.DataFrame(list(data_dict.items()), columns=[\"attribute\", \"context\"])\n",
    "\n",
    "# Ensure the 'attribute' column in the output matches the column names in dataset\n",
    "output[\"attribute\"] = pd.Categorical(\n",
    "    output[\"attribute\"], categories=dataset.columns.str.strip(), ordered=True\n",
    ")\n",
    "\n",
    "# Sort the DataFrame according to the order in the array_of_strings\n",
    "output = output.sort_values(\"attribute\")\n",
    "\n",
    "# Write the DataFrame to CSV without extra quotes or escape characters\n",
    "output.to_csv(\"./output/context/context_output.csv\", index=False, quoting=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context Refinement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = pd.read_csv(\"./output/context/context_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"math_response\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"output\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"attribute_index\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"The index in the list where the attribute column description occurs\",\n",
    "                            },\n",
    "                            \"context\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The enriched contextual description of each attribute column\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"attribute_index\",\n",
    "                            \"context\",\n",
    "                        ],\n",
    "                        \"additionalProperties\": False,\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"output\"],\n",
    "            \"additionalProperties\": False,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_refinement_prompt = \"\"\"You are given contextual descriptions of the columns of a relational dataset. You have to analyze these descriptions and enrhich them by taking the context of all columns into consideration.\n",
    "\n",
    "The context of the entire table is derived with the following steps:\n",
    "\n",
    "1. Determine dependencies between columns.\n",
    "- Certain columns may have dependecies, like date of birth and age, or state code and city.\n",
    "Example:\n",
    "A column that is suspected to have state codes could indicated that there is a possible dependency with columns that stores city names.\n",
    "Example:\n",
    "The first three letters of US social security numbers indicate the area where the code was issued, and could indicate a hidden dependency with a column like area code or area name. Always check non natural lanuage text fields for such patterns.\n",
    "\n",
    "2. View the columns as a whole. \n",
    "- When you consider all of the columns in a dataset it could give an indication on the meaning of the columns. \n",
    "Example:\n",
    "If a dataset contains a column that is a positvie percentage, a negative percentage, a monetary value, and a date it is likely data on a stock portfolio. This information could be used to enrhich the context of the columns.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_enriched_string(context: pd.DataFrame) -> str:\n",
    "    context_delimited = context.drop(context.columns[0], axis=1).reset_index()\n",
    "    context_delimited[context_delimited.columns[1]] = context_delimited[\n",
    "        context_delimited.columns[1]\n",
    "    ].apply(lambda x: f\"|{x}|\")\n",
    "\n",
    "    context_delimited[context_delimited.columns[0]] = context_delimited[\n",
    "        context_delimited.columns[0]\n",
    "    ].apply(lambda x: f\"[{x}]\")\n",
    "\n",
    "    context_delimited_string = context_delimited.to_csv(\n",
    "        index=False, header=False, lineterminator=\"\\n\", sep=\">\"\n",
    "    )\n",
    "\n",
    "    return context_delimited_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\enriched_context\\context_refinement\n",
      "Processing complete. Results and token/time data have been saved.\n"
     ]
    }
   ],
   "source": [
    "directory = r\"D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\enriched_context\"\n",
    "\n",
    "user_prompt = f\"\"\"Input:\n",
    "The first column of the input is an index and is delimited with \"[index]\". The context that has to be enriched is in the second column and delimited with \"|context|\".\n",
    "The relationship between an index is indicated using \" > \".\n",
    "A index value pair is indicated by \"[index]>|context|\".\n",
    "{generate_context_enriched_string(context)}\n",
    "\"\"\"\n",
    "\n",
    "prompt_gpt(\n",
    "    context_refinement_prompt,\n",
    "    user_prompt,\n",
    "    \"context_refinement\",\n",
    "    response_format,\n",
    "    directory,\n",
    "    context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r\"D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\enriched_context\\context_refinement\\output.json\"\n",
    "\n",
    "\n",
    "with open(directory, \"r\") as json_file:\n",
    "    try:\n",
    "        json_data = json.load(json_file)\n",
    "        # Retrieve the annotation and index from the json output and create a dataframe\n",
    "        context = [item[\"context\"] for item in json_data[\"output\"]]\n",
    "        attribute_index = [item[\"attribute_index\"] for item in json_data[\"output\"]]\n",
    "        # print(file_path)\n",
    "        # print(index)\n",
    "        attribute_context = pd.DataFrame(\n",
    "            {\"attribute_index\": attribute_index, \"context\": context}\n",
    "        )\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON from file {file_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Level Prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"math_response\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"output\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"explanation\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The explanation for why a value is considered an error\",\n",
    "                            },\n",
    "                            \"index\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"The index in the list where the value occurs\",\n",
    "                            },\n",
    "                            \"annotation\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"The annotation denoting whether a value is an error or not\",\n",
    "                                \"enum\": [1, 0],\n",
    "                            },\n",
    "                            \"possible_repair\": {\"type\": \"string\"},\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"explanation\",\n",
    "                            \"index\",\n",
    "                            \"annotation\",\n",
    "                            \"possible_repair\",\n",
    "                        ],\n",
    "                        \"additionalProperties\": False,\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"output\"],\n",
    "            \"additionalProperties\": False,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_prompt = \"\"\"You are given a list of numeric values with their corresponding index. You have to identify all errors in the list.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompt = \"\"\"You are given a list of unique values in a column with their corresponding index. You have to find all syntactic errors in the dataset and recommend a possible repair.\n",
    "A syntactic error occurs when a value does not conform to the structure or domain of correct values. The domain and structure of correct values have to be derived from the values themselves.\n",
    "A semantic error occurs when a value falls outside of the reasonable context of a column. Use the context description to determine if a value is a semantic error.\n",
    "You have to annotate an error with a '1' and a correct value of the '0' in the output.\n",
    "For the possible repair only provide the reparied value is output.\n",
    "You also have to provide a brief explanation referencing the examples a proof for each annotation.\n",
    "Evaluate each value and provide an annotation and explanation regardless of error status.\n",
    "Values denoting empty or null values can be found in any given context, and are considered correct.\n",
    "Note! Only check for syntactic errors. Do not check for language errors.\n",
    "\n",
    "Syntactic deviations can be one of the following examples\n",
    "1. Invalid characters\n",
    "- Characters appear in values that do not often appear in others or make them uninterpretable\n",
    "Example 1\n",
    "1, John = 0\n",
    "2, Greg = 0\n",
    "3, Frank15 = 1\n",
    "\n",
    "Example 2 \n",
    "15, Apple = 0\n",
    "16, Pxar = 1\n",
    "17, Banana = 0\n",
    "\n",
    "2. Misspelling\n",
    "- Words that are misspelled. Values that are considered names are less likely to be misspelt.\n",
    "Example 1\n",
    "2, Blue = 0\n",
    "3, Green = 0\n",
    "4, Orage = 1\n",
    "\n",
    "Example 2 \n",
    "67, Reservation for two people near the window = 0\n",
    "68, Reservatton for five poeple at the entrance = 1\n",
    "69, Three humans arriving at 9 for drinks = 0\n",
    "70, A single peersonn at three = 1\n",
    "\n",
    "3. Pattern non-conformity\n",
    "- Some values may have a common pattern with certain values deviating from this pattern. There might exist more than one valid pattern in a single attribute.\n",
    "Possible repairs should attempt to conform with the most prevalent patterns. Removing a pattern does not constitute a soluition.\n",
    "Example 1\n",
    "32, 2024/03/12 19:00 = 0\n",
    "33, 2024-12-31 12:00 = 1\n",
    "34, 1994/01/13 15:15 = 0\n",
    "35, 12:00am 3 January 2024 = 1\n",
    "\n",
    "Example 2\n",
    "70, Admin123 = 0\n",
    "71, Bob443 = 0\n",
    "72, 99Alex = 1\n",
    "\"\"\"\n",
    "\n",
    "# And a context description of the attribute.\n",
    "# 4. Contextual meaning\n",
    "# - The contextual meaning of a column could provide some clues as to the expected values in a column.\n",
    "# Example 1\n",
    "# Contextual meaning: A column possibly storing state codes\n",
    "# 22, MA = 0\n",
    "# 23, TX = 0\n",
    "# 24, ZA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_attribute_prompt_string(attribute_unique) -> str:\n",
    "    attribute_delimited = attribute_unique[\n",
    "        [attribute_unique.columns[1], attribute_unique.columns[0]]\n",
    "    ]\n",
    "\n",
    "    if attribute_delimited[attribute_unique.columns[0]].dtype != \"int64\":\n",
    "        attribute_delimited.loc[:, attribute_unique.columns[0]] = attribute_delimited[\n",
    "            attribute_unique.columns[0]\n",
    "        ].apply(lambda x: x.strip())\n",
    "\n",
    "    attribute_delimited.loc[:, attribute_unique.columns[0]] = attribute_delimited[\n",
    "        attribute_unique.columns[0]\n",
    "    ].apply(lambda x: f\"|{x}|\")\n",
    "\n",
    "    attribute_delimited.loc[:, attribute_unique.columns[1]] = attribute_delimited[\n",
    "        attribute_unique.columns[1]\n",
    "    ].apply(lambda x: f\"[{x}]\")\n",
    "\n",
    "    attribute_unique_string = attribute_delimited.to_csv(\n",
    "        index=False, header=False, lineterminator=\"\\n\", sep=\">\"\n",
    "    )\n",
    "\n",
    "    return attribute_unique_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for col in dataset.columns:\n",
    "    attribute = pd.DataFrame(dataset[col].copy())\n",
    "    context = attribute_context[attribute_context[\"attribute_index\"] == count][\n",
    "        \"context\"\n",
    "    ].values[0]\n",
    "    count += 1\n",
    "\n",
    "    attribute_dict, attribute_unique = create_attribute_dict(attribute, col)\n",
    "\n",
    "    if attribute[attribute.columns[0]].dtype == object:\n",
    "        system_prompt = text_prompt\n",
    "    else:\n",
    "        system_prompt = numeric_prompt\n",
    "\n",
    "    attribute_unique.columns = [\"value\", \"index\"]\n",
    "    json_sample = attribute_unique.to_json(orient=\"records\", indent=4)\n",
    "\n",
    "    # Context:\n",
    "    # The context is delimited as \"#Context#'\n",
    "    # #{context}#\n",
    "\n",
    "    user_prompt = f\"\"\"Input:\n",
    "    {json_sample}\n",
    "\"\"\"\n",
    "\n",
    "    #     The first column of the input is an index and does not have to be checked for errors and is delimited with \"[index]\". The data to be checked is in the second column and delimited with \"|value|\".\n",
    "    # The relationship between an index is indicated using \" > \".\n",
    "    # A index value pair is indicated by \"[index]>|value|\".\n",
    "    # {generate_attribute_prompt_string(attribute_unique)}\n",
    "\n",
    "    directory = r\"D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\attribute_output\"\n",
    "\n",
    "    # os.makedirs(directory, exist_ok=True)\n",
    "    # file_path = os.path.join(directory, col.strip(), \"dict.csv\")\n",
    "\n",
    "    # # Save the attribute dictionary to a CSV file\n",
    "    # attribute_dict.to_csv(\n",
    "    #     file_path, index=False, header=True, lineterminator=\"\\n\", sep=\",\"\n",
    "    # )\n",
    "\n",
    "    prompt_gpt(\n",
    "        system_prompt,\n",
    "        user_prompt,\n",
    "        col,\n",
    "        response_format,\n",
    "        directory,\n",
    "        attribute_dict,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store data for each folder\n",
    "data_dict = {}\n",
    "\n",
    "# Loop through all subdirectories and files\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    # Get the folder name (which will be the column name)\n",
    "    folder_name = os.path.basename(root)\n",
    "\n",
    "    # Skip the root directory itself (attribute_output)\n",
    "    if root == directory:\n",
    "        continue\n",
    "\n",
    "    annotated_output = None\n",
    "    dict_data = None\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            # Open and load the JSON file\n",
    "            with open(file_path, \"r\") as json_file:\n",
    "                try:\n",
    "                    json_data = json.load(json_file)\n",
    "                    # Retrieve the annotation and index from the json output and create a dataframe\n",
    "                    annotations = [item[\"annotation\"] for item in json_data[\"output\"]]\n",
    "                    index = [item[\"index\"] for item in json_data[\"output\"]]\n",
    "\n",
    "                    annotated_output = pd.DataFrame(\n",
    "                        {\"annotation\": annotations, \"index\": index}\n",
    "                    )\n",
    "\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON from file {file_path}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "        elif file == \"dict.csv\":\n",
    "            dict_file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                dict_data = pd.read_csv(dict_file_path)\n",
    "                dict_data.columns = dict_data.columns.str.strip()\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading dict.csv from file {dict_file_path}: {e}\")\n",
    "\n",
    "    if annotated_output is not None and dict_data is not None:\n",
    "        annotated_output = annotated_output.drop_duplicates(\n",
    "            subset=\"index\", keep=\"first\"\n",
    "        )\n",
    "\n",
    "        dict_out_merged = pd.merge(\n",
    "            dict_data,\n",
    "            annotated_output,\n",
    "            left_on=\"unique_index\",\n",
    "            right_on=\"index\",\n",
    "            how=\"left\",\n",
    "        )  # Join unique indices to the original values\n",
    "\n",
    "        dict_out_merged.fillna(0, inplace=True)\n",
    "        data_dict[folder_name] = dict_out_merged[\"annotation\"]\n",
    "    else:\n",
    "        print(f\"Missing files for folder {folder_name}\")\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "output = pd.DataFrame(data_dict)\n",
    "output = output[dataset.columns.str.strip()]  # Strip whitespace from columns\n",
    "output = output.astype(int)\n",
    "output.to_csv(\"./attribute_output/output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_csv(\"./attribute_output/output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_annotation = pd.read_csv(\n",
    "    \"./datasets/hospital/errors.csv\", header=None, index_col=None, skiprows=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df1, df2):\n",
    "    # Flatten the dataframes to 1D arrays\n",
    "    y_true = df1.values.flatten()\n",
    "    y_pred = df2.values.flatten()\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Precision\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "\n",
    "    # Recall\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "\n",
    "    f_score = f1_score(y_true, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9838\n",
      "Precision: 0.8192534381139489\n",
      "Recall: 0.6425269645608629\n",
      "F1 score: 0.7202072538860104\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f_score = calculate_metrics(output, error_annotation)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 score: {f_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_classification(error_annotation, output, input):\n",
    "    error_annotation.columns = input.columns\n",
    "    output.columns = input.columns\n",
    "\n",
    "    calc = error_annotation.add(2)\n",
    "    calc_out = output\n",
    "    calc_out[calc_out == 0] = -1\n",
    "\n",
    "    calc = calc.add(calc_out)\n",
    "\n",
    "    # True positive calculation\n",
    "    tp = calc == 4\n",
    "    tp = input[tp]\n",
    "\n",
    "    # False positive calculation\n",
    "    fp = calc == 3\n",
    "    fp = input[fp]\n",
    "\n",
    "    # False negative calculation\n",
    "    fn = calc == 2\n",
    "    fn = input[fn]\n",
    "\n",
    "    return tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, fn = inspect_classification(\n",
    "    error_annotation=error_annotation,\n",
    "    output=output,\n",
    "    input=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.to_csv(\"./attribute_output/tp.csv\")\n",
    "fp.to_csv(\"./attribute_output/fp.csv\")\n",
    "fn.to_csv(\"./attribute_output/fn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"math_response\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"output\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"columns\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\"type\": \"number\"},\n",
    "                                \"description\": \"An array of columns that might share a dependency.\",\n",
    "                            },\n",
    "                            \"dependency\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"A description of the dependency between the identified columns.\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"columns\", \"dependency\"],\n",
    "                        \"additionalProperties\": False,\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"output\"],\n",
    "            \"additionalProperties\": False,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are given a sample of ten records from a relational database. You have to determine which columns might have dependencies between them. \n",
    "For the output, only provide pairs of columns that might have dependencies. If multipule dependencies exist with a single column, provide more than one outputs for that column.\n",
    "\n",
    "Dependencies can occur in two ways.\n",
    "\n",
    "1. Semantic dependency\n",
    "- The meaning of the values of one column determines the meaning of another. \n",
    "Example:\n",
    "One column represents cities, and another countries. This may indicated that there is a dependency between the two columns, and that the citities column, contains cities that are present in that country.\n",
    "Col 1                   , Col 2\n",
    "Great Britain           , London\n",
    "United States of America, Washington DC\n",
    "South Africa            , Pretoria\n",
    "\n",
    "Note! Other semantic dependecies may occur in different domains from the one mentioned in the example.\n",
    "\n",
    "2. Pattern Dependency\n",
    "- One column may have a pattern, which in part is based on the meaning of another column. \n",
    "Example:\n",
    "One column represents emergency codes, the other emergency descriptions.\n",
    "Col 1      , Col 2\n",
    "FRE-003_a  , Forrest fire with damages above $2,000\n",
    "FLD-001_z  , Floods that destroyed local buildings\n",
    "ERQ-777_n  , Earthquakes that caused power outages \n",
    "HUR-008_t  , Hurricanes that was able to breach sea walls\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency\\output\n",
      "Processing complete. Results and token/time data have been saved.\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "directory = (\n",
    "    r\"D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency\"\n",
    ")\n",
    "\n",
    "dataset_sample = dataset.sample(n=10)\n",
    "# dataset_sample = dataset\n",
    "dataset_sample.columns = range(dataset_sample.shape[1])\n",
    "json_sample = dataset_sample.to_json(orient=\"records\", indent=4)\n",
    "# print(json_sample)\n",
    "# attribute_unique_string = attribute_delimited.to_csv(\n",
    "#         index=False, header=False, lineterminator=\"\\n\", sep=\">\"\n",
    "#     )\n",
    "# dataset_sample.to_csv(index=False, lineterminator=\"\\n\", sep=\">\")\n",
    "# formatted_table = tabulate(\n",
    "#     dataset_sample, headers=\"keys\", tablefmt=\"pretty\", showindex=False\n",
    "# )\n",
    "\n",
    "user_prompt = f\"\"\"Input:\n",
    "The following is a formatted table with the data to be checked.\n",
    "{json_sample}\n",
    "\"\"\"\n",
    "\n",
    "prompt_gpt(\n",
    "    system_prompt,\n",
    "    user_prompt,\n",
    "    \"output\",\n",
    "    response_format,\n",
    "    directory,\n",
    "    dataset_sample,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r\"D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency\\output\\output.json\"\n",
    "\n",
    "\n",
    "with open(directory, \"r\") as json_file:\n",
    "    try:\n",
    "        json_data = json.load(json_file)\n",
    "        # Retrieve the annotation and index from the json output and create a dataframe\n",
    "        columns = [item[\"columns\"] for item in json_data[\"output\"]]\n",
    "        dependency = [item[\"dependency\"] for item in json_data[\"output\"]]\n",
    "        # print(file_path)\n",
    "        # print(index)\n",
    "        dependencies = pd.DataFrame({\"columns\": columns, \"dependency\": dependency})\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON from file {file_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"math_response\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"output\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"explanation\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The explanation for why a value is considered an error\",\n",
    "                            },\n",
    "                            \"index\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"The index in the list where the value occurs\",\n",
    "                            },\n",
    "                            \"column\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"The column where the violation occured\",\n",
    "                            },\n",
    "                            \"annotation\": {\n",
    "                                \"type\": \"number\",\n",
    "                                \"description\": \"The annotation denoting whether a value is an error or not\",\n",
    "                                \"enum\": [1, 0],\n",
    "                            },\n",
    "                            \"possible_repair\": {\"type\": \"string\"},\n",
    "                        },\n",
    "                        \"required\": [\n",
    "                            \"explanation\",\n",
    "                            \"index\",\n",
    "                            \"annotation\",\n",
    "                            \"possible_repair\",\n",
    "                            \"column\",\n",
    "                        ],\n",
    "                        \"additionalProperties\": False,\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"output\"],\n",
    "            \"additionalProperties\": False,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are given data from columns in a dataset that is said to have a dependency between each other. You have to detect violations in this dependency.\n",
    "\n",
    "You are also given a description of the dependency that you can use to help you identify violations.\n",
    "\n",
    "For the possible repair provide only the repair value.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dependency_prompt_string(columns: pd.DataFrame) -> str:\n",
    "    # Create a true copy of the DataFrame to avoid warnings about modifying slices\n",
    "    data_columns = columns.copy()\n",
    "\n",
    "    # Remove duplicate rows based on all columns\n",
    "    unique_combinations = data_columns.drop_duplicates(\n",
    "        subset=data_columns.columns.tolist()\n",
    "    ).copy()\n",
    "\n",
    "    # Add index as a new column, and format it with brackets using .loc[]\n",
    "    unique_combinations[\"index\"] = (\n",
    "        unique_combinations.index\n",
    "    )  # Ensure you're adding a new column\n",
    "    unique_combinations[\"index\"] = unique_combinations[\"index\"].apply(\n",
    "        lambda x: f\"[{x}]\"\n",
    "    )\n",
    "\n",
    "    # Apply delimiters to other columns\n",
    "    for col in unique_combinations.columns:\n",
    "        if col != \"index\":  # Skip the 'index' column\n",
    "            unique_combinations.loc[:, col] = unique_combinations[col].apply(\n",
    "                lambda x: f\"|{x}|\"\n",
    "            )\n",
    "\n",
    "    # Reorder columns to move 'index' to the front\n",
    "    cols = [\"index\"] + [col for col in unique_combinations.columns if col != \"index\"]\n",
    "    unique_combinations = unique_combinations[cols]\n",
    "\n",
    "    # Convert the DataFrame to CSV format with the required delimiters\n",
    "    unique_combinations_string = unique_combinations.to_csv(\n",
    "        index=False, header=False, lineterminator=\"\\n\", sep=\",\"\n",
    "    )\n",
    "\n",
    "    return unique_combinations_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\0\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\1\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\2\n",
      "Processing complete. Results and token/time data have been saved.\n",
      "Output and prompt saved in directory: D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\\3\n",
      "Processing complete. Results and token/time data have been saved.\n"
     ]
    }
   ],
   "source": [
    "for index, dependency in dependencies.iterrows():\n",
    "\n",
    "    data_columns = dataset[dataset.columns[dependency.columns]]\n",
    "    # unique_combinations = data_columns.drop_duplicates()\n",
    "    string = generate_dependency_prompt_string(data_columns)\n",
    "    directory = r\"D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\"\n",
    "\n",
    "    column_string = \"|\".join(str(num) for num in dependency.columns)\n",
    "    # column_string = \"| col:\".join(map(str, dependency.columns))\n",
    "    # print(column_string)\n",
    "\n",
    "    data_columns.columns = dependency.columns\n",
    "    json_sample = data_columns.to_json(orient=\"records\", indent=4)\n",
    "\n",
    "    user_prompt = f\"\"\"Input:\n",
    "The dependency identified in this table is defined as follows:\n",
    "{dependency.dependency}   \n",
    "\n",
    "The following is a formatted table with the data to be checked.\n",
    "{json_sample}\n",
    "\"\"\"\n",
    "    #     [index],|{column_string}|\n",
    "    # {string}\n",
    "\n",
    "    prompt_gpt(\n",
    "        system_prompt,\n",
    "        user_prompt,\n",
    "        str(index),\n",
    "        response_format,\n",
    "        directory,\n",
    "        data_columns,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the directory containing the JSON files\n",
    "directory = r\"D:\\Documents\\UU\\Thesis\\Artifact\\CAED\\dataset_analyzer\\notebook\\output\\dependency_violations\"\n",
    "\n",
    "# Assuming the original DataFrame structure is known\n",
    "original_dataframe = dataset  # Replace with your actual DataFrame\n",
    "\n",
    "# Initialize the DataFrame with zeros\n",
    "annotated_output = pd.DataFrame(\n",
    "    0, index=original_dataframe.index, columns=original_dataframe.columns\n",
    ")\n",
    "\n",
    "# Loop through all subdirectories and files\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    # Get the folder name (which will be the column name)\n",
    "    folder_name = os.path.basename(root)\n",
    "\n",
    "    # Skip the root directory itself (attribute_output)\n",
    "    if root == directory:\n",
    "        continue\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            # Open and load the JSON file\n",
    "            with open(file_path, \"r\") as json_file:\n",
    "                try:\n",
    "                    json_data = json.load(json_file)\n",
    "\n",
    "                    # Retrieve the annotation, index, and column from the json output\n",
    "                    annotations = [item[\"annotation\"] for item in json_data[\"output\"]]\n",
    "                    indices = [item[\"index\"] for item in json_data[\"output\"]]\n",
    "                    columns = [item[\"column\"] for item in json_data[\"output\"]]\n",
    "\n",
    "                    # Create a temporary DataFrame to store the current annotations\n",
    "                    current_annotations = pd.DataFrame(\n",
    "                        {\"annotation\": annotations, \"index\": indices, \"column\": columns}\n",
    "                    )\n",
    "\n",
    "                    # Update the annotated output DataFrame\n",
    "                    for _, row in current_annotations.iterrows():\n",
    "                        index = row[\"index\"]\n",
    "                        column = row[\"column\"]\n",
    "                        annotation = row[\"annotation\"]\n",
    "\n",
    "                        # Ensure the index is within bounds\n",
    "                        if index < len(original_dataframe):\n",
    "                            # Get the column name from the index\n",
    "                            col_name = original_dataframe.columns[column]\n",
    "\n",
    "                            # Replace the original data with the annotation (if 1, else keep original)\n",
    "                            if annotation == 1:  # Only replace if there's an error\n",
    "                                annotated_output.at[index, col_name] = {annotation}\n",
    "                            else:\n",
    "                                annotated_output.at[index, col_name] = (\n",
    "                                    0  # Fill with 0 if no error\n",
    "                                )\n",
    "\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON from file {file_path}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file_path}: {e}\")\n",
    "\n",
    "# Save to CSV\n",
    "annotated_output.to_csv(\"./dependency_violations/output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, fn = inspect_classification(\n",
    "    error_annotation=error_annotation,\n",
    "    output=annotated_output,\n",
    "    input=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.to_csv(\"./output/dependency_violations/tp.csv\")\n",
    "fp.to_csv(\"./output/dependency_violations/fp.csv\")\n",
    "fn.to_csv(\"./output/dependency_violations/fn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the two datasets\n",
    "dataset_1 = pd.read_csv(\n",
    "    \"./output/dependency_violations/dependency_output.csv\"\n",
    ")  # Path to first dataset\n",
    "dataset_2 = pd.read_csv(\n",
    "    \"./output/attribute_output/output.csv\"\n",
    ")  # Path to second dataset\n",
    "\n",
    "# Ensure both datasets have the same structure (columns and index)\n",
    "# Initialize a consolidated DataFrame with the same shape as the original datasets\n",
    "consolidated_data = pd.DataFrame(0, index=dataset_1.index, columns=dataset_1.columns)\n",
    "\n",
    "# Loop through the indices and columns to consolidate annotations in place\n",
    "for col in dataset_1.columns:\n",
    "    for index in dataset_1.index:\n",
    "        # Check if the current index has an error in dataset 1\n",
    "        error_1 = dataset_1.at[index, col]\n",
    "        error_2 = (\n",
    "            dataset_2.at[index, col] if index < len(dataset_2) else 0\n",
    "        )  # Handle cases where index exceeds\n",
    "\n",
    "        # Logic to consolidate annotations\n",
    "        if error_1 == 1 or error_2 == 1:  # If either dataset has an error\n",
    "            consolidated_data.at[index, col] = 1  # Mark as error\n",
    "        else:\n",
    "            consolidated_data.at[index, col] = 0  # No error\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "consolidated_data.to_csv(\"./output/consolidated_error_annotations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93515\n",
      "Precision: 0.8722986247544204\n",
      "Recall: 0.2649164677804296\n",
      "F1 score: 0.4064073226544622\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f_score = calculate_metrics(\n",
    "    consolidated_data, error_annotation\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 score: {f_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, fn = inspect_classification(\n",
    "    error_annotation=error_annotation,\n",
    "    output=output,\n",
    "    input=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.to_csv(\"./output/tp.csv\")\n",
    "fp.to_csv(\"./output/fp.csv\")\n",
    "fn.to_csv(\"./output/fn.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
