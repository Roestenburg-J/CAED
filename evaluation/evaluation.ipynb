{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_1 = \"flights_20241114_101306\"\n",
    "output_file_2 = \"flights_20241114_103536\"\n",
    "output_file_3 = \"flights_20241114_101306\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consolidated Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_1}/consolidated_error_annotations.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "output_2 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_2}/consolidated_error_annotations.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "output_3 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_3}/consolidated_error_annotations.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset = pd.read_csv(f\"../backend/data/{output_file_1}/clean.csv\")\n",
    "dirty_dataset = pd.read_csv(f\"../backend/data/{output_file_1}/dirty.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_errors(\n",
    "    df_fixed: pd.DataFrame, df_with_errors: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    # Check if the dataframes have the same shape and columns after sorting\n",
    "    if df_fixed.shape != df_with_errors.shape or not all(\n",
    "        df_fixed.columns == df_with_errors.columns\n",
    "    ):\n",
    "        raise ValueError(\"Both dataframes must have the same structure.\")\n",
    "\n",
    "    # Convert both dataframes to strings for datatype-agnostic comparison\n",
    "    df_fixed_str = df_fixed.astype(str)\n",
    "    df_with_errors_str = df_with_errors.astype(str)\n",
    "\n",
    "    # Create the annotation dataframe by comparing the two dataframes\n",
    "    error_annotation = (df_fixed_str != df_with_errors_str).astype(int)\n",
    "\n",
    "    return error_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_annotation = annotate_errors(clean_dataset, dirty_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_method(df1, df2, df3):\n",
    "    \"\"\"\n",
    "    Combine three dataframes using the union method.\n",
    "    A cell is 1 in the output if it is 1 in any of the input dataframes.\n",
    "    \"\"\"\n",
    "    return (df1 | df2 | df3).astype(int)\n",
    "\n",
    "\n",
    "def threshold_method(df1, df2, df3, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Combine three dataframes using a threshold.\n",
    "    A cell is 1 in the output if it is 1 in at least `threshold` proportion of the input dataframes.\n",
    "    \"\"\"\n",
    "    # Stack the dataframes and calculate the sum along the stack\n",
    "    stacked = np.stack([df1.values, df2.values, df3.values])\n",
    "    count_ones = np.sum(stacked, axis=0)\n",
    "\n",
    "    # Calculate the threshold in terms of number of dataframes\n",
    "    threshold_count = int(threshold * 3)\n",
    "\n",
    "    # Determine if each cell meets the threshold and create a DataFrame\n",
    "    result = (count_ones >= threshold_count).astype(int)\n",
    "\n",
    "    # Return the result as a DataFrame with the same index and columns as the input dataframes\n",
    "    return pd.DataFrame(result, index=df1.index, columns=df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "\n",
    "def calculate_metrics(true_dataset: pd.DataFrame, pred_dataset: pd.DataFrame):\n",
    "    # Flatten the dataframes to 1D arrays\n",
    "    y_true = true_dataset.values.flatten()\n",
    "    y_pred = pred_dataset.values.flatten()\n",
    "\n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Handle precision, recall, and F1 score gracefully\n",
    "    try:\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    except ValueError:\n",
    "        # This case happens when y_true has no positive samples\n",
    "        precision = recall = f1 = 0.0\n",
    "\n",
    "    # Class-specific accuracy\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if cm.size == 4:  # If confusion matrix has 4 elements (2x2 matrix)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    else:  # Handle cases where there's only one class in y_true\n",
    "        tn = fp = fn = tp = 0\n",
    "        if len(cm) == 1:\n",
    "            tn = cm[0, 0] if y_true[0] == 0 else 0\n",
    "            tp = cm[0, 0] if y_true[0] == 1 else 0\n",
    "\n",
    "    # AUC scores\n",
    "    if len(set(y_true)) > 1:  # AUC scores require at least two classes\n",
    "        roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    else:\n",
    "        roc_auc = None\n",
    "\n",
    "    # Count of 1s in true and predicted labels\n",
    "    predicted_positives_count = sum(y_pred == 1)  # Total 1s in true labels\n",
    "    actual_positives_count = sum(y_true == 1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1_score\": float(f1),\n",
    "        \"roc_auc\": float(roc_auc) if roc_auc is not None else None,\n",
    "        \"true_positives_count\": int(tp),\n",
    "        \"true_negative_count\": int(tn),\n",
    "        \"false_positive_count\": int(fp),\n",
    "        \"false_negative_count\": int(fn),\n",
    "        \"predicted_positives_count\": int(predicted_positives_count),\n",
    "        \"actual_positives_count\": int(actual_positives_count),\n",
    "        \"fp_rate\": float(fp / len(y_pred)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-wise metric calculation wrapper\n",
    "def calculate_columnwise_metrics(\n",
    "    true_dataset: pd.DataFrame, pred_dataset: pd.DataFrame\n",
    "):\n",
    "    results = {}\n",
    "\n",
    "    for column in true_dataset.columns:\n",
    "        if column in pred_dataset.columns:\n",
    "\n",
    "            metrics = calculate_metrics(true_dataset[[column]], pred_dataset[[column]])\n",
    "            results[column] = metrics\n",
    "        else:\n",
    "            results[column] = \"Column missing in predictions\"\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_output = union_method(output_1, output_2, output_3)\n",
    "\n",
    "threshold_output = threshold_method(output_1, output_2, output_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5573593073593074, 'precision': 0.3241647465437788, 'recall': 0.45752032520325203, 'f1_score': 0.37946729602157786, 'roc_auc': 0.5284100942956151, 'true_positives_count': 2251, 'true_negative_count': 7019, 'false_positive_count': 4693, 'false_negative_count': 2669, 'predicted_positives_count': 6944, 'actual_positives_count': 4920, 'fp_rate': 0.2821669071669072}\n"
     ]
    }
   ],
   "source": [
    "print(calculate_metrics(error_annotation, union_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5573593073593074, 'precision': 0.3241647465437788, 'recall': 0.45752032520325203, 'f1_score': 0.37946729602157786, 'roc_auc': 0.5284100942956151, 'true_positives_count': 2251, 'true_negative_count': 7019, 'false_positive_count': 4693, 'false_negative_count': 2669, 'predicted_positives_count': 6944, 'actual_positives_count': 4920, 'fp_rate': 0.2821669071669072}\n"
     ]
    }
   ],
   "source": [
    "print(calculate_metrics(error_annotation, threshold_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_columnwise_metrics(error_annotation, union_output)\n",
    "\n",
    "pd.DataFrame(results).to_csv(\"./consolidated_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = pd.read_csv(f\"../backend/data/{output_file_1}/attribute/output.csv\")\n",
    "\n",
    "output_2 = pd.read_csv(f\"../backend/data/{output_file_2}/attribute/output.csv\")\n",
    "\n",
    "output_3 = pd.read_csv(f\"../backend/data/{output_file_3}/attribute/output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_output = union_method(output_1, output_2, output_3)\n",
    "\n",
    "threshold_output = threshold_method(output_1, output_2, output_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.545995670995671, 'precision': 0.12252510760401722, 'recall': 0.08678861788617886, 'f1_score': 0.10160618679357526, 'roc_auc': 0.4128444455551113, 'true_positives_count': 427, 'true_negative_count': 8654, 'false_positive_count': 3058, 'false_negative_count': 4493, 'predicted_positives_count': 3485, 'actual_positives_count': 4920, 'fp_rate': 0.18386243386243387}\n"
     ]
    }
   ],
   "source": [
    "print(calculate_metrics(error_annotation, union_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.545995670995671, 'precision': 0.12252510760401722, 'recall': 0.08678861788617886, 'f1_score': 0.10160618679357526, 'roc_auc': 0.4128444455551113, 'true_positives_count': 427, 'true_negative_count': 8654, 'false_positive_count': 3058, 'false_negative_count': 4493, 'predicted_positives_count': 3485, 'actual_positives_count': 4920, 'fp_rate': 0.18386243386243387}\n"
     ]
    }
   ],
   "source": [
    "print(calculate_metrics(error_annotation, threshold_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_columnwise_metrics(error_annotation, union_output)\n",
    "\n",
    "pd.DataFrame(results).to_csv(\"./attribute_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency Violation Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_1}/dependency_violations/output.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "output_2 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_2}/dependency_violations/output.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "output_3 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_3}/dependency_violations/output.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_output = union_method(output_1, output_2, output_3)\n",
    "\n",
    "threshold_output = threshold_method(output_1, output_2, output_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7285954785954786, 'precision': 0.5507753876938469, 'recall': 0.4475609756097561, 'f1_score': 0.49383269791433054, 'roc_auc': 0.6471069905371184, 'true_positives_count': 2202, 'true_negative_count': 9916, 'false_positive_count': 1796, 'false_negative_count': 2718, 'predicted_positives_count': 3998, 'actual_positives_count': 4920, 'fp_rate': 0.10798460798460799}\n"
     ]
    }
   ],
   "source": [
    "print(calculate_metrics(error_annotation, union_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7285954785954786, 'precision': 0.5507753876938469, 'recall': 0.4475609756097561, 'f1_score': 0.49383269791433054, 'roc_auc': 0.6471069905371184, 'true_positives_count': 2202, 'true_negative_count': 9916, 'false_positive_count': 1796, 'false_negative_count': 2718, 'predicted_positives_count': 3998, 'actual_positives_count': 4920, 'fp_rate': 0.10798460798460799}\n"
     ]
    }
   ],
   "source": [
    "print(calculate_metrics(error_annotation, threshold_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_columnwise_metrics(error_annotation, union_output)\n",
    "\n",
    "pd.DataFrame(results).to_csv(\"./dep_viol_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totals for each dataset:\n",
      "   completion_tokens  prompt_tokens  total_tokens  batches  duration_seconds  \\\n",
      "0            25461.0       145152.0      170613.0     48.0           214.507   \n",
      "1            29767.0       183880.0      213647.0     60.0           260.720   \n",
      "2            34225.0       200045.0      234270.0     58.0           278.790   \n",
      "\n",
      "    duration  \n",
      "0  03:34.507  \n",
      "1  04:20.720  \n",
      "2  04:38.790  \n",
      "\n",
      "Average Duration Across Datasets:\n",
      "04:11.339\n",
      "Total Tokens:  206176.66666666666\n",
      "Completion Tokens:  29817.666666666668\n",
      "Prompt Tokens:  176359.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# measure = \"attribute\"\n",
    "# measure = \"dependency\"\n",
    "measure = \"dependency_violations\"\n",
    "\n",
    "\n",
    "# Load your DataFrames\n",
    "df1 = pd.read_csv(\n",
    "    f\"../backend/data/rayyan_20241114_105931/{measure}/prompt_metadata.csv\"\n",
    ")\n",
    "df2 = pd.read_csv(\n",
    "    f\"../backend/data/rayyan_20241114_111119/{measure}/prompt_metadata.csv\"\n",
    ")\n",
    "df3 = pd.read_csv(\n",
    "    f\"../backend/data/rayyan_20241114_112300/{measure}/prompt_metadata.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "# Convert duration from MM:SS.MSS to seconds\n",
    "def duration_to_seconds(duration):\n",
    "    try:\n",
    "        # Split minutes and seconds\n",
    "        minutes, seconds = duration.split(\":\")\n",
    "        minutes = int(minutes)\n",
    "        seconds = float(seconds)  # Includes fractional seconds\n",
    "        return minutes * 60 + seconds\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing duration '{duration}': {e}\")\n",
    "        return 0  # Default to 0 seconds for invalid formats\n",
    "\n",
    "\n",
    "# Apply conversion to all DataFrames\n",
    "for df in [df1, df2, df3]:\n",
    "    df[\"duration_seconds\"] = df[\"elapsed_time\"].apply(duration_to_seconds)\n",
    "\n",
    "\n",
    "# Calculate totals for each dataset\n",
    "def calculate_totals(df):\n",
    "    totals = df.sum(numeric_only=True)  # Sum only numeric columns\n",
    "    totals[\"duration_seconds\"] = df[\"duration_seconds\"].sum()\n",
    "    return totals\n",
    "\n",
    "\n",
    "totals1 = calculate_totals(df1)\n",
    "totals2 = calculate_totals(df2)\n",
    "totals3 = calculate_totals(df3)\n",
    "\n",
    "# Combine totals into a single DataFrame\n",
    "totals_df = pd.DataFrame([totals1, totals2, totals3])\n",
    "\n",
    "# Calculate the average across datasets\n",
    "averages = totals_df.mean()\n",
    "\n",
    "\n",
    "# Convert total duration per dataset and average duration to MM:SS.MSS format\n",
    "def seconds_to_duration(seconds):\n",
    "    minutes = int(seconds // 60)\n",
    "    seconds = seconds % 60\n",
    "    return f\"{minutes:02}:{seconds:06.3f}\"  # Keeps milliseconds in the output\n",
    "\n",
    "\n",
    "# Convert durations\n",
    "totals_df[\"duration\"] = totals_df[\"duration_seconds\"].apply(seconds_to_duration)\n",
    "average_duration = seconds_to_duration(averages[\"duration_seconds\"])\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"Totals for each dataset:\")\n",
    "print(totals_df)\n",
    "print(\"\\nAverage Duration Across Datasets:\")\n",
    "print(average_duration)\n",
    "print(\"Total Tokens: \", averages[\"total_tokens\"])\n",
    "print(\"Completion Tokens: \", averages[\"completion_tokens\"])\n",
    "print(\"Prompt Tokens: \", averages[\"prompt_tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novel Detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C\n",
      "0  0.0  0.0  5.0\n",
      "1  0.0  0.0  0.0\n",
      "2  3.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example DataFrames\n",
    "df1 = pd.DataFrame({\"A\": [1, 0, 3], \"B\": [0, 4, 0], \"C\": [5, 0, 0]})\n",
    "\n",
    "df2 = pd.DataFrame({\"A\": [1, 0, 0], \"B\": [0, 4, 2], \"C\": [0, 1, 0]})\n",
    "\n",
    "# Step 1: Find where the values differ\n",
    "difference_mask = df1 != df2\n",
    "\n",
    "# Step 2: Create a DataFrame with only differing values\n",
    "differences = df1.where(difference_mask, np.nan)\n",
    "\n",
    "# Step 3: (Optional) Replace NaN with 0 if you want a 0-filled output\n",
    "differences_filled = differences.fillna(0)\n",
    "\n",
    "\n",
    "print(differences_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ 0.026453849999999998\n"
     ]
    }
   ],
   "source": [
    "tokens = 176359\n",
    "cost = 0.15\n",
    "\n",
    "print(\"$\", float(tokens / 1000000) * cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ 0.0178908\n"
     ]
    }
   ],
   "source": [
    "tokens = 29818\n",
    "cost = 0.60\n",
    "\n",
    "print(\"$\", float(tokens / 1000000) * cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_classification(\n",
    "    true_dataset: pd.DataFrame, pred_dataset: pd.DataFrame, input_dataset: pd.DataFrame\n",
    "):\n",
    "    true_dataset.reset_index(drop=True)\n",
    "    pred_dataset.reset_index(drop=True)\n",
    "    input_dataset.reset_index(drop=True)\n",
    "\n",
    "    true_dataset.columns = input_dataset.columns\n",
    "    pred_dataset.columns = input_dataset.columns\n",
    "\n",
    "    calc = true_dataset.add(2)\n",
    "    calc_out = pred_dataset.copy()\n",
    "    calc_out[calc_out == 0] = -1\n",
    "\n",
    "    calc = calc.add(calc_out)\n",
    "\n",
    "    # True positive calculation\n",
    "    tp = calc == 4\n",
    "    true_positive_df = input_dataset[tp].astype(str)\n",
    "    true_positive_df = true_positive_df.replace(to_replace=\"nan\", value=0)\n",
    "    true_positive_df = true_positive_df.reset_index(drop=True)  # Remove index\n",
    "\n",
    "    # False positive calculation\n",
    "    fp = calc == 3\n",
    "    false_positive_df = input_dataset[fp].astype(str)\n",
    "    false_positive_df = false_positive_df.replace(to_replace=\"nan\", value=0)\n",
    "    false_positive_df = false_positive_df.reset_index(drop=True)  # Remove index\n",
    "\n",
    "    # False negative calculation\n",
    "    fn = calc == 2\n",
    "    false_negative_df = input_dataset[fn].astype(str)\n",
    "    false_negative_df = false_negative_df.replace(to_replace=\"nan\", value=0)\n",
    "    false_negative_df = false_negative_df.reset_index(drop=True)  # Remove index\n",
    "\n",
    "    all_errors_df = input_dataset[fp | fn].astype(str)\n",
    "    all_errors_df = all_errors_df.replace(to_replace=\"nan\", value=0)\n",
    "    all_errors_df = all_errors_df.reset_index(drop=True)  # Remove index\n",
    "\n",
    "    return true_positive_df, false_positive_df, false_negative_df, all_errors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_1 = \"rayyan_20241114_105931\"\n",
    "output_file_2 = \"rayyan_20241114_111119\"\n",
    "output_file_3 = \"rayyan_20241114_112300\"\n",
    "\n",
    "output_1 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_1}/consolidated_error_annotations.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "output_2 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_2}/consolidated_error_annotations.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "output_3 = pd.read_csv(\n",
    "    f\"../backend/data/{output_file_3}/consolidated_error_annotations.csv\"\n",
    ")\n",
    "\n",
    "union_output = union_method(output_1, output_2, output_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tools/raha/datasets/rayyan/annotated_cells.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrayyan\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m raha \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m----> 4\u001b[0m     pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tools/raha/datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/annotated_cells.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# pd.read_csv(f\"./tools/raha/datasets/movies_1/annotated_cells.csv\")\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# SynODC output\u001b[39;00m\n\u001b[0;32m      9\u001b[0m syn \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     10\u001b[0m     pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tools/SynODC/Results/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/output/annotated_output.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     13\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Janus\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Janus\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Janus\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\Janus\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Janus\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tools/raha/datasets/rayyan/annotated_cells.csv'"
     ]
    }
   ],
   "source": [
    "dataset = \"rayyan\"\n",
    "\n",
    "raha = (\n",
    "    pd.read_csv(f\"./tools/raha/datasets/{dataset}/annotated_cells.csv\")\n",
    "    # pd.read_csv(f\"./tools/raha/datasets/movies_1/annotated_cells.csv\")\n",
    "    .astype(int).fillna(0)\n",
    ")\n",
    "# SynODC output\n",
    "syn = (\n",
    "    pd.read_csv(f\"./tools/SynODC/Results/{dataset}/output/annotated_output.csv\")\n",
    "    .astype(int)\n",
    "    .fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raha_synodc = (raha | syn).fillna(0)\n",
    "\n",
    "# Make sure the columns are in the correct order and ensure no index changes\n",
    "raha_synodc = raha_synodc.astype(int)\n",
    "\n",
    "# If you want to ensure the column order is preserved explicitly, you can reorder:\n",
    "raha_synodc = raha_synodc[raha.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_dataset = pd.read_csv(f\"./datasets/{dataset}/{dataset}.csv\")\n",
    "clean_dataset = pd.read_csv(f\"./datasets/{dataset}/clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_annotation = annotate_errors(clean_dataset, dirty_dataset)\n",
    "\n",
    "# CAED output\n",
    "true_positive_df, false_positive_df, false_negative_df, all_errors_df = (\n",
    "    inspect_classification(error_annotation, union_output, dirty_dataset)\n",
    ")\n",
    "\n",
    "caed_tp = true_positive_df.copy()\n",
    "\n",
    "\n",
    "# Raha SynODC output\n",
    "true_positive_df, false_positive_df, false_negative_df, all_errors_df = (\n",
    "    inspect_classification(error_annotation, raha_synodc, dirty_dataset)\n",
    ")\n",
    "\n",
    "raha_syn_tp = true_positive_df.copy()\n",
    "\n",
    "# Raha output\n",
    "true_positive_df, false_positive_df, false_negative_df, all_errors_df = (\n",
    "    inspect_classification(error_annotation, raha, dirty_dataset)\n",
    ")\n",
    "\n",
    "raha_tp = true_positive_df.copy()\n",
    "\n",
    "# SynODC output\n",
    "true_positive_df, false_positive_df, false_negative_df, all_errors_df = (\n",
    "    inspect_classification(error_annotation, syn, dirty_dataset)\n",
    ")\n",
    "\n",
    "syn_tp = true_positive_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of non-zero values: 253\n"
     ]
    }
   ],
   "source": [
    "# Combined\n",
    "# Step 1: Find where the values differ\n",
    "difference_mask = caed_tp != raha_syn_tp\n",
    "\n",
    "# Step 2: Create a DataFrame with only differing values\n",
    "differences = caed_tp.where(difference_mask, np.nan)\n",
    "\n",
    "# Step 3: Replace NaN with 0, ensuring all values are treated as float\n",
    "differences_filled = differences.fillna(0)\n",
    "\n",
    "# Optional: Drop the first column if needed\n",
    "# differences_filled.drop(columns=differences_filled.columns[0], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Step 4: Define a function to identify non-zero values robustly\n",
    "def is_non_zero(value):\n",
    "    try:\n",
    "        # Try to cast to a float, compare to zero\n",
    "        return float(value) != 0.0\n",
    "    except ValueError:\n",
    "        # If value cannot be converted to a float, assume it's non-zero\n",
    "        return True\n",
    "\n",
    "\n",
    "# Apply the function to the entire DataFrame\n",
    "non_zero_mask = differences_filled.applymap(is_non_zero)\n",
    "\n",
    "# Count the number of True values (non-zero values)\n",
    "non_zero_count = non_zero_mask.sum().sum()\n",
    "\n",
    "print(f\"Count of non-zero values: {non_zero_count}\")\n",
    "\n",
    "# Step 5: Save the differences\n",
    "differences_filled.to_csv(f\"./novel_detections/{dataset}/novel.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of non-zero values Raha: 256\n"
     ]
    }
   ],
   "source": [
    "# Raha output\n",
    "\n",
    "# Step 1: Find where the values differ\n",
    "difference_mask = caed_tp != raha_tp\n",
    "\n",
    "\n",
    "# Step 2: Create a DataFrame with only differing values\n",
    "differences = caed_tp.where(difference_mask, np.nan)\n",
    "\n",
    "\n",
    "# Step 3: Replace NaN with 0, ensuring all values are treated as float\n",
    "differences_filled = differences.fillna(0)\n",
    "\n",
    "\n",
    "# Optional: Drop the first column if needed\n",
    "# differences_filled.drop(columns=differences_filled.columns[0], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Step 4: Define a function to identify non-zero values robustly\n",
    "def is_non_zero(value):\n",
    "    try:\n",
    "        # Try to cast to a float, compare to zero\n",
    "        return float(value) != 0.0\n",
    "    except ValueError:\n",
    "        # If value cannot be converted to a float, assume it's non-zero\n",
    "        return True\n",
    "\n",
    "\n",
    "# Apply the function to the entire DataFrame\n",
    "non_zero_mask = differences_filled.applymap(is_non_zero)\n",
    "\n",
    "# Count the number of True values (non-zero values)\n",
    "non_zero_count = non_zero_mask.sum().sum()\n",
    "\n",
    "print(f\"Count of non-zero values Raha: {non_zero_count}\")\n",
    "\n",
    "# Step 5: Save the differences\n",
    "# differences_filled.to_csv(\"./novel.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of non-zero values SynODC: 516\n"
     ]
    }
   ],
   "source": [
    "# SynODC\n",
    "\n",
    "# Raha output\n",
    "# raha = pd.read_csv(f\"./tools/raha/datasets/{dataset}/tp.csv\")\n",
    "# # SynODC output\n",
    "\n",
    "\n",
    "# raha_syn = raha.combine_first(syn).iloc[:, 1:]\n",
    "\n",
    "\n",
    "# Step 1: Find where the values differ\n",
    "difference_mask = caed_tp != syn_tp\n",
    "\n",
    "# Step 2: Create a DataFrame with only differing values\n",
    "differences = caed_tp.where(difference_mask, np.nan)\n",
    "\n",
    "# Step 3: Replace NaN with 0, ensuring all values are treated as float\n",
    "differences_filled = differences.fillna(0)\n",
    "\n",
    "# Optional: Drop the first column if needed\n",
    "# differences_filled.drop(columns=differences_filled.columns[0], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Step 4: Define a function to identify non-zero values robustly\n",
    "def is_non_zero(value):\n",
    "    try:\n",
    "        # Try to cast to a float, compare to zero\n",
    "        return float(value) != 0.0\n",
    "    except ValueError:\n",
    "        # If value cannot be converted to a float, assume it's non-zero\n",
    "        return True\n",
    "\n",
    "\n",
    "# Apply the function to the entire DataFrame\n",
    "non_zero_mask = differences_filled.applymap(is_non_zero)\n",
    "\n",
    "# Count the number of True values (non-zero values)\n",
    "non_zero_count = non_zero_mask.sum().sum()\n",
    "\n",
    "print(f\"Count of non-zero values SynODC: {non_zero_count}\")\n",
    "\n",
    "# Step 5: Save the differences\n",
    "# differences_filled.to_csv(\"./novel.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
