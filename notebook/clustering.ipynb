{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import gower\n",
    "\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./datasets/hospital/dirty.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine if a column is categorical based on frequency of unique values\n",
    "def is_categorical(column, threshold=0.2):\n",
    "    unique_values = column.nunique()\n",
    "    total_values = len(column)\n",
    "    return (unique_values / total_values) < threshold\n",
    "\n",
    "\n",
    "# Function to extract initial categories based on frequency threshold\n",
    "def extract_categories(column, frequency_threshold=0.02):\n",
    "    # Get the value counts for the column\n",
    "    category_counts = column.value_counts(\n",
    "        normalize=True\n",
    "    )  # Normalize to get percentages\n",
    "    # Return the categories that appear more than the frequency threshold (as a ratio)\n",
    "    return category_counts[category_counts >= frequency_threshold].index.tolist()\n",
    "\n",
    "\n",
    "# Function to clean categorical values using fuzzy matching\n",
    "def clean_categorical_values(column, known_categories, similarity_threshold=85):\n",
    "    cleaned_column = []\n",
    "\n",
    "    for value in column:\n",
    "        # Skip if the value is NaN\n",
    "        if pd.isna(value):\n",
    "            cleaned_column.append(value)\n",
    "            continue\n",
    "\n",
    "        # Find the categorical value that best matches the value in question\n",
    "        match_result = process.extractOne(value, known_categories, scorer=fuzz.ratio)\n",
    "\n",
    "        # Check if a match was found\n",
    "        if match_result:\n",
    "            match, score, _ = match_result\n",
    "\n",
    "            # If the similarity score is above the threshold, replace with the matched category\n",
    "            if score >= similarity_threshold:\n",
    "                cleaned_column.append(match)\n",
    "            else:\n",
    "                cleaned_column.append(value)  # Keep original if no good match\n",
    "        else:\n",
    "            cleaned_column.append(value)  # Keep original if no match found\n",
    "\n",
    "    return cleaned_column\n",
    "\n",
    "\n",
    "# Function to scale numerical columns between 0 and 1\n",
    "def scale_numerical_columns(column):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_column = scaler.fit_transform(column.values.reshape(-1, 1)).flatten()\n",
    "    return scaled_column\n",
    "\n",
    "\n",
    "# Function to process the entire dataframe\n",
    "def clean_and_encode_df(df):\n",
    "    cleaned_df = df.copy()\n",
    "\n",
    "    for column in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            # print(f\"Scaling numerical column: {column}\")\n",
    "            # Scale the numerical column\n",
    "            cleaned_df[column] = scale_numerical_columns(df[column])\n",
    "        elif df[column].dtype == \"object\":  # Only work with text columns\n",
    "            if is_categorical(df[column]):\n",
    "                # print(f\"Processing categorical column: {column}\")\n",
    "                # Extract categories based on frequency\n",
    "                categories = extract_categories(df[column])\n",
    "                # print(f\"Extracted categories for '{column}': {categories}\")\n",
    "\n",
    "                # Clean the column based on similarity to categories\n",
    "                cleaned_values = clean_categorical_values(df[column], categories)\n",
    "                cleaned_df[column] = cleaned_values\n",
    "\n",
    "                # Apply label encoding to the cleaned categorical values\n",
    "                # le = LabelEncoder()\n",
    "                # cleaned_df[column] = le.fit_transform(cleaned_df[column])\n",
    "                # print(\n",
    "                #     f\"Encoded values for '{column}': {dict(zip(le.classes_, range(len(le.classes_))))}\"\n",
    "                # )\n",
    "            else:\n",
    "                # print(f\"Column '{column}' is not categorical (freeform text).\")\n",
    "                cleaned_df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "# Apply the cleaning and encoding to the dataset\n",
    "cleaned_df = clean_and_encode_df(dataset)\n",
    "cleaned_df.to_csv(\"./clustering/prerpoccessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1 - Cluster Data using DBSCAN and visualize with PCA, t-SNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply DBSCAN to a dataset\n",
    "def apply_dbscan(df, eps, min_samples, dist_metric=None):\n",
    "    # Normalize the features\n",
    "\n",
    "    # Fit DBSCAN\n",
    "    if dist_metric.shape[0] > 0:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric=\"precomputed\")\n",
    "        df[\"Cluster\"] = dbscan.fit_predict(dist_metric)\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_features = scaler.fit_transform(df)\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        df[\"Cluster\"] = dbscan.fit_predict(scaled_features)\n",
    "\n",
    "    return df, dbscan\n",
    "\n",
    "\n",
    "def apply_kmeans(df, n_clusters):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_features = scaler.fit_transform(df)\n",
    "\n",
    "    # Fit K-Means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(scaled_features)\n",
    "    df[\"Cluster\"] = labels\n",
    "\n",
    "    return df, kmeans\n",
    "\n",
    "\n",
    "# After determining the optimal number of clusters from the elbow plot (for example, if k=3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize clusters using PCA\n",
    "def visualize_clusters_pca(df, cluster_column):\n",
    "\n",
    "    df_without_clusters = df.drop(columns=[cluster_column])\n",
    "    pca = PCA(n_components=2)\n",
    "    components = pca.fit_transform(df_without_clusters.select_dtypes(include=\"number\"))\n",
    "\n",
    "    pca_df = pd.DataFrame(data=components, columns=[\"PCA1\", \"PCA2\"])\n",
    "    pca_df[cluster_column] = df[cluster_column].values\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.scatterplot(\n",
    "        data=pca_df, x=\"PCA1\", y=\"PCA2\", hue=cluster_column, palette=\"viridis\", s=100\n",
    "    )\n",
    "    plt.title(f\"PCA of Clusters ({cluster_column})\")\n",
    "    plt.legend(title=cluster_column)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to visualize clusters using t-SNE\n",
    "def visualize_clusters_tsne(df, cluster_column):\n",
    "\n",
    "    df_without_clusters = df.drop(columns=[cluster_column])\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    tsne_components = tsne.fit_transform(\n",
    "        df_without_clusters.select_dtypes(include=\"number\")\n",
    "    )\n",
    "\n",
    "    tsne_df = pd.DataFrame(data=tsne_components, columns=[\"TSNE1\", \"TSNE2\"])\n",
    "    tsne_df[cluster_column] = df[cluster_column].values\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.scatterplot(\n",
    "        data=tsne_df, x=\"TSNE1\", y=\"TSNE2\", hue=cluster_column, palette=\"viridis\", s=100\n",
    "    )\n",
    "    plt.title(f\"t-SNE of Clusters ({cluster_column})\")\n",
    "    plt.legend(title=cluster_column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize DBSCAN clusters using PCA and t-SNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process the entire dataframe\n",
    "def clean_and_encode_df_tsne(df):\n",
    "    cleaned_df = df.copy()\n",
    "\n",
    "    for column in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            # print(f\"Scaling numerical column: {column}\")\n",
    "            # Scale the numerical column\n",
    "            cleaned_df[column] = scale_numerical_columns(df[column])\n",
    "        elif df[column].dtype == \"object\":  # Only work with text columns\n",
    "            if is_categorical(df[column]):\n",
    "                # print(f\"Processing categorical column: {column}\")\n",
    "                # Extract categories based on frequency\n",
    "                categories = extract_categories(df[column])\n",
    "                # print(f\"Extracted categories for '{column}': {categories}\")\n",
    "\n",
    "                # Clean the column based on similarity to categories\n",
    "                cleaned_values = clean_categorical_values(df[column], categories)\n",
    "                cleaned_df[column] = cleaned_values\n",
    "\n",
    "                # Apply label encoding to the cleaned categorical values\n",
    "                le = LabelEncoder()\n",
    "                cleaned_df[column] = le.fit_transform(cleaned_df[column])\n",
    "                # print(\n",
    "                #     f\"Encoded values for '{column}': {dict(zip(le.classes_, range(len(le.classes_))))}\"\n",
    "                # )\n",
    "            else:\n",
    "                # print(f\"Column '{column}' is not categorical (freeform text).\")\n",
    "                cleaned_df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "# Apply the cleaning and encoding to the dataset\n",
    "cleaned_df_tsne = clean_and_encode_df(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply DBSCAN to data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = cleaned_df.dropna()\n",
    "# Convert numeric columns to float\n",
    "numeric_cols = cleaned_df.select_dtypes(include=[np.number]).columns\n",
    "cleaned_df[numeric_cols] = cleaned_df[numeric_cols].astype(np.float64)\n",
    "\n",
    "gower_dist = gower.gower_matrix(cleaned_df)\n",
    "df_dbscan, dbscan_model = apply_dbscan(\n",
    "    cleaned_df, eps=0.1, min_samples=5, dist_metric=gower_dist\n",
    ")\n",
    "\n",
    "df_dbscan.to_csv(\"./clustering/dbscan.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_clusters_pca(df_dbscan, \"Cluster\")\n",
    "cleaned_df_tsne = clean_and_encode_df_tsne(dataset)\n",
    "cleaned_df_tsne[\"Cluster\"] = df_dbscan[\"Cluster\"]\n",
    "visualize_clusters_tsne(cleaned_df_tsne, \"Cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize k-means clusters using PCA, t-SNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans, kmeans_model = apply_kmeans(cleaned_df_tsne, n_clusters=6)\n",
    "\n",
    "df_kmeans.to_csv(\"./clustering/k-means.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_clusters_pca(df_kmeans, \"Cluster\")\n",
    "visualize_clusters_tsne(df_kmeans, \"Cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2 - Apply PCA and then perform Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply PCA to the dataset\n",
    "def apply_pca(data, n_components=2):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principal_components = pca.fit_transform(data)\n",
    "\n",
    "    # Create a DataFrame with the PCA results\n",
    "    pca_df = pd.DataFrame(\n",
    "        data=principal_components, columns=[f\"PC{i+1}\" for i in range(n_components)]\n",
    "    )\n",
    "    print(\n",
    "        f\"Explained variance by each principal component: {pca.explained_variance_ratio_}\"\n",
    "    )\n",
    "\n",
    "    return pca_df, pca\n",
    "\n",
    "\n",
    "def plot_clusters(data):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Scatter plot with different clusters\n",
    "    plt.scatter(\n",
    "        data[\"PC1\"], data[\"PC2\"], c=data[\"Cluster\"], cmap=\"plasma\", s=50, alpha=0.7\n",
    "    )\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.title(\"DBSCAN Clusters after PCA\")\n",
    "    plt.colorbar(label=\"Cluster\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dbscan(df, eps, min_samples):\n",
    "    # Normalize the features\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_features = scaler.fit_transform(df)\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    df[\"Cluster\"] = dbscan.fit_predict(scaled_features)\n",
    "\n",
    "    return df, dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Principal Components from the data\n",
    "pca_df, pca_model = apply_pca(cleaned_df_tsne, n_components=2)\n",
    "\n",
    "# Cluster Using DBSCAN\n",
    "pca_dbscan, pca_dbscan_model = apply_dbscan(pca_df, eps=0.05, min_samples=20)\n",
    "\n",
    "# Plot the clusters\n",
    "plot_clusters(pca_dbscan)\n",
    "\n",
    "# Optionally, save the clustered data to a CSV\n",
    "pca_dbscan.to_csv(\"./pca_dbscan.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot elbow\n",
    "def elbow_method(data, max_clusters=10):\n",
    "    data_without_clusters = data.drop(columns=\"Cluster\")\n",
    "    wcss = []\n",
    "\n",
    "    # Fit KMeans with a range of cluster numbers and compute WCSS (Within-Cluster Sum of Squares)\n",
    "    for i in range(1, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "        kmeans.fit(data_without_clusters)\n",
    "        wcss.append(kmeans.inertia_)  # inertia_ is the WCSS\n",
    "\n",
    "    # Plot the elbow graph\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, max_clusters + 1), wcss, marker=\"o\", linestyle=\"-\", color=\"b\")\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"WCSS (Within-Cluster Sum of Squares)\")\n",
    "    plt.title(\"Elbow Method for Optimal K\")\n",
    "    plt.xticks(range(1, max_clusters + 1))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize k-means\n",
    "def plot_clusters(data, kmeans_model):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Scatter plot with different clusters\n",
    "    plt.scatter(\n",
    "        data[\"PC1\"], data[\"PC2\"], c=data[\"Cluster\"], cmap=\"viridis\", s=50, alpha=0.7\n",
    "    )\n",
    "\n",
    "    # Mark the cluster centers\n",
    "    # centers = kmeans_model.cluster_centers_\n",
    "    # plt.scatter(\n",
    "    #     centers[:, 0],\n",
    "    #     centers[:, 1],\n",
    "    #     c=\"red\",\n",
    "    #     s=200,\n",
    "    #     alpha=0.75,\n",
    "    #     marker=\"X\",\n",
    "    #     label=\"Centroids\",\n",
    "    # )\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.title(f\"KMeans Clusters (k={kmeans_model.n_clusters}) after PCA\")\n",
    "    plt.legend()\n",
    "    plt.colorbar(label=\"Cluster\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the clusters (using k=3 in this example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_method(pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_kmeans, kmeans_model = apply_kmeans(pca_df, n_clusters=3)\n",
    "\n",
    "pca_kmeans.to_csv(\"./clustering/pca_k-means.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters(pca_kmeans, kmeans_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def grid_search_dbscan(df, eps_values, min_samples_values):\n",
    "#     best_score = -1\n",
    "#     best_params = None\n",
    "\n",
    "#     # Iterate over all combinations of eps and min_samples\n",
    "#     for eps in eps_values:\n",
    "#         for min_samples in min_samples_values:\n",
    "#             print(f\"Evaluating DBSCAN with eps={eps}, min_samples={min_samples}\")\n",
    "\n",
    "#             # Apply DBSCAN\n",
    "#             clustered_df, _ = apply_dbscan(df, eps=eps, min_samples=min_samples)\n",
    "#             labels = clustered_df[\"DBSCAN_Cluster\"]\n",
    "\n",
    "#             # Check if there are enough clusters to calculate silhouette score\n",
    "#             if len(set(labels)) > 1 and len(set(labels)) < len(labels):\n",
    "#                 score = silhouette_score(df, labels)\n",
    "#                 print(f\"Silhouette Score: {score:.3f}\")\n",
    "\n",
    "#                 # Update the best score and parameters if the current score is better\n",
    "#                 if score > best_score:\n",
    "#                     best_score = score\n",
    "#                     best_params = (eps, min_samples)\n",
    "\n",
    "#     return best_params, best_score\n",
    "\n",
    "\n",
    "# # Define the parameter grid\n",
    "# eps_values = np.logspace(-2, 0, num=10)  # eps from 0.01 to 1.0\n",
    "# min_samples_values = range(2, 10)  # min_samples from 2 to 9\n",
    "\n",
    "# # Apply the grid search\n",
    "# best_params, best_score = grid_search_dbscan(cleaned_df, eps_values, min_samples_values)\n",
    "\n",
    "# # Output the best parameters\n",
    "# print(\"Best Parameters:\", best_params)\n",
    "# print(\"Best Silhouette Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "\n",
    "# def grid_search_dbscan(df, eps_values, min_samples_values):\n",
    "#     best_score = float(\"inf\")  # Initialize to infinity for Davies-Bouldin\n",
    "#     best_params = None\n",
    "\n",
    "#     # Iterate over all combinations of eps and min_samples\n",
    "#     for eps in eps_values:\n",
    "#         for min_samples in min_samples_values:\n",
    "#             print(f\"Evaluating DBSCAN with eps={eps}, min_samples={min_samples}\")\n",
    "\n",
    "#             # Apply DBSCAN\n",
    "#             clustered_df, _ = apply_dbscan(df, eps=eps, min_samples=min_samples)\n",
    "#             labels = clustered_df[\"DBSCAN_Cluster\"]\n",
    "\n",
    "#             # Check if there are enough clusters to calculate Davies-Bouldin Index\n",
    "#             if len(set(labels)) > 1 and len(set(labels)) < len(labels):\n",
    "#                 # Calculate Davies-Bouldin Index\n",
    "#                 score = davies_bouldin_score(df, labels)\n",
    "#                 print(f\"Davies-Bouldin Index: {score:.3f}\")\n",
    "\n",
    "#                 # Update the best score and parameters if the current score is better (lower is better)\n",
    "#                 if score < best_score:\n",
    "#                     best_score = score\n",
    "#                     best_params = (eps, min_samples)\n",
    "\n",
    "#     return best_params, best_score\n",
    "\n",
    "\n",
    "# # Define the parameter grid\n",
    "# eps_values = np.logspace(-2, 0, num=10)  # eps from 0.01 to 1.0\n",
    "# min_samples_values = range(2, 10)  # min_samples from 2 to 9\n",
    "\n",
    "# # Apply the grid search\n",
    "# best_params, best_score = grid_search_dbscan(cleaned_df, eps_values, min_samples_values)\n",
    "\n",
    "# # Output the best parameters\n",
    "# print(\"Best Parameters:\", best_params)\n",
    "# print(\"Best Davies-Bouldin Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "\n",
    "# Function to apply Kernel PCA to the dataset\n",
    "def apply_kpca(data, n_components=2, kernel=\"rbf\", gamma=None):\n",
    "    kpca = KernelPCA(n_components=n_components, kernel=kernel, gamma=gamma)\n",
    "    principal_components = kpca.fit_transform(data)\n",
    "\n",
    "    # Create a DataFrame with the KPCA results\n",
    "    kpca_df = pd.DataFrame(\n",
    "        data=principal_components, columns=[f\"PC{i+1}\" for i in range(n_components)]\n",
    "    )\n",
    "\n",
    "    return kpca_df, kpca\n",
    "\n",
    "\n",
    "def plot_clusters_kpca(data):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Scatter plot with different clusters\n",
    "    plt.scatter(\n",
    "        data[\"PC1\"], data[\"PC2\"], c=data[\"Cluster\"], cmap=\"plasma\", s=50, alpha=0.7\n",
    "    )\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.title(\"DBSCAN Clusters after Kernel PCA\")\n",
    "    plt.colorbar(label=\"Cluster\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(data):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Scatter plot with different clusters\n",
    "    plt.scatter(\n",
    "        data[\"PC1\"], data[\"PC2\"], c=data[\"Cluster\"], cmap=\"plasma\", s=50, alpha=0.7\n",
    "    )\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.title(\"DBSCAN Clusters after PCA\")\n",
    "    plt.colorbar(label=\"Cluster\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2  # Number of components to keep\n",
    "kpca_df, kpca_model = apply_kpca(cleaned_df_tsne, n_components=n_components)\n",
    "# print(kpca_df)\n",
    "\n",
    "# Cluster Using DBSCAN\n",
    "kpca_dbscan, kpca_dbscan_model = apply_dbscan(kpca_df, eps=0.2, min_samples=2)\n",
    "\n",
    "# Plot the clusters\n",
    "plot_clusters(kpca_dbscan)\n",
    "\n",
    "# Optionally, save the clustered data to a CSV\n",
    "# pca_dbscan.to_csv(\"./pca_dbscan.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
